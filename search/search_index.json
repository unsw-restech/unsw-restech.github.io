{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#katana-shared-computational-cluster-at-unsw","title":"Katana - Shared Computational Cluster at UNSW","text":"<p>Katana is a computational cluster at UNSW with over 6,000 CPU cores, 8 GPU compute nodes (V100 and A100),  and 6Pb of disk storage allowing users to run jobs not feasible on personal devices because they take too long, require too much memory, there is too much data, there is data shared between multiple people, or just too many calculations that need to be run. See details for more information about Katana.</p> <p>Information on how to request a Katana account can be found with the information on how to access Katana.</p> <p></p>"},{"location":"#are-you-looking-for","title":"Are you looking for....","text":"Web based access to Katana Restech training (including Katana) Getting help and support"},{"location":"#katana-terms-of-use","title":"Katana Terms of Use","text":"<p>Any use of Katana is covered by the Conditions of Use - UNSW ICT Resources. </p> <p>Warning</p> <p>Katana is NOT suitable for sensitive or highly sensitive data. You should use the UNSW Data Classification scheme to classify your data and learn about managing your research data by visiting the Research Data Management Hub.</p>"},{"location":"AI_in_Katana/StableDiffusion/","title":"Stable Diffusion on an HPC Environment","text":""},{"location":"AI_in_Katana/StableDiffusion/#stable-diffusion-on-an-hpc-environment","title":"Stable Diffusion on an HPC Environment","text":"<p>This documentation covers how to run Stable Diffusion on a High-Performance Computing (HPC) environment, including sections on installation, inference using job scheduling, and finetuning a model.</p>"},{"location":"AI_in_Katana/StableDiffusion/#1-installation","title":"1. Installation","text":"<ol> <li> <p>Create the Working Directory    Create a directory for the Stable Diffusion setup on the scratch space (or any suitable storage on your HPC). This is where all necessary files and environments will reside:    </p><pre><code>export base_dir=\"/srv/scratch/$USER/&lt;your directory&gt;\"\nmkdir -p $base_dir\ncd $base_dir\n</code></pre><p></p> </li> <li> <p>Load the Python Module    Ensure you have Python installed on your HPC and load the appropriate module (adjust based on your HPC environment):    </p><pre><code>module load python\n</code></pre><p></p> </li> <li> <p>Create and Activate a Virtual Environment    Create a Python virtual environment for Stable Diffusion and its dependencies:    </p><pre><code>python -m venv venv\nsource venv/bin/activate\n</code></pre><p></p> </li> <li> <p>Install Required Dependencies    Install the necessary packages, such as <code>torch</code>, <code>diffusers</code>, and other essential libraries:    </p><pre><code>pip install torch diffusers transformers accelerate pillow tqdm pyyaml\n</code></pre><p></p> </li> </ol>"},{"location":"AI_in_Katana/StableDiffusion/#2-inference-using-job-scheduling","title":"2. Inference using Job Scheduling","text":"<p>To perform inference on an HPC, job scheduling is essential to allocate resources efficiently and run tasks on available nodes. For #PBS arguments, check out running jobs on Katana</p> <ol> <li>Create a PBS Script    A PBS script schedules the job to run on the HPC. Here's a sample script to run inference with a Stable Diffusion model:</li> </ol> <pre><code>#!/bin/bash\n#PBS -l select=1:ncpus=4:ngpus=1:mem=16GB:gpu_model=V100\n#PBS -l walltime=2:00:00\n#PBS -j oe\n#PBS -N sd-inference\n#PBS -M &lt;your_email&gt;@unsw.edu.au\n#PBS -m abe\n\n# Load necessary modules\nmodule load python\n\n# Activate your virtual environment\nsource $base_dir/venv/bin/activate\n\n# Set Hugging Face cache to scratch\nexport HF_HOME=\"/srv/scratch/$USER/huggingface_cache\"\n\n# Define arguments for inference\nPROMPT=\"A serene landscape with mountains and a lake\"\nMODEL_ID=\"CompVis/stable-diffusion-v1-4\"\nLORA_CHECKPOINT=\"/srv/scratch/$USER/finetune/sd-flower-model/&lt;your checkpoint&gt;\" # leave it blank if you do not have checkpoint\n\n# Run the inference Python script\npython inference.py --prompt \"$PROMPT\" --model_id \"$MODEL_ID\" --lora_checkpoint \"$LORA_CHECKPOINT\"\n</code></pre> <p>Note</p> <p>Important: Set the Hugging Face Cache Directory (HF_HOME) By default, Hugging Face's models and checkpoints are saved in the home directory cache, which is often limited in space (e.g., 10GB). On HPC systems, the scratch directory usually has more space (e.g., 120GB), but it is not backed up. You should configure Hugging Face to store its cache in the scratch directory by setting the <code>HF_HOME</code> environment variable:</p> <pre><code>export HF_HOME=\"/srv/scratch/&lt;your_user&gt;/huggingface_cache\"\n</code></pre> <p>This will ensure that models and related files are stored in your scratch directory where you have ample space. Important: After completing your work, it is recommended to move any important models or files from scratch to your home directory, as scratch space is not backed up.</p> <ol> <li>Inference Script (<code>inference.py</code>)    The inference script loads a pre-trained Stable Diffusion model, optionally applies a LoRA checkpoint, and generates images based on the input prompt.</li> </ol> <pre><code>import argparse\nimport torch\nfrom diffusers import StableDiffusionPipeline\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\ndef inference(prompt, model_id, lora_checkpoint=None):\n    # Load the model\n    pipeline = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    pipeline.to(device)\n\n    # Apply LoRA checkpoint if available\n    if lora_checkpoint:\n        pipeline.unet.load_attn_procs(lora_checkpoint)\n\n    # Generate images\n    num_samples = 4\n    generator = torch.Generator(device).manual_seed(42)\n    images = pipeline(\n        prompt,\n        num_inference_steps=30,\n        generator=generator,\n        num_images_per_prompt=num_samples,\n        height=512,\n        width=512,\n        guidance_scale=7.5\n    ).images\n\n    # Arrange images in a grid and save\n    grid_image = make_grid(images, rows=2, cols=2)\n    grid_image.save(f\"image_grid_{prompt.replace(' ', '_')}.png\")\n\ndef make_grid(images, rows, cols):\n    w, h = images[0].size\n    grid = Image.new('RGB', size=(cols * w, rows * h))\n    for i, img in enumerate(images):\n        grid.paste(img, box=(i % cols * w, i // cols * h))\n    return grid\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Stable Diffusion Inference\")\n    parser.add_argument(\"--prompt\", type=str, required=True)\n    parser.add_argument(\"--model_id\", type=str, required=True)\n    parser.add_argument(\"--lora_checkpoint\", type=str, help=\"Path to LoRA checkpoint, if any\")\n    args = parser.parse_args()\n\n    inference(args.prompt, args.model_id, args.lora_checkpoint)\n</code></pre> <ol> <li>Submit the Job    Use the <code>qsub</code> command to submit the PBS job:    <pre><code>qsub &lt;pbs_script_name&gt;.pbs\n</code></pre></li> </ol>"},{"location":"AI_in_Katana/StableDiffusion/#3-finetuning","title":"3. Finetuning","text":"<p>Finetuning Stable Diffusion with your dataset on an HPC requires adjusting the model weights to fit the specific domain of your new data.</p> <ol> <li> <p>Prepare Dataset     Check out the Hugging Face documentation on datasets to find out how to create metadata and place dataset in a folder in a way that is compatible with model finetuning. </p> </li> <li> <p>Transfer dataset to Katana     Depending on your OS, you can use either <code>rsync</code> command or <code>WinSCP</code> software or <code>Globus</code></p> </li> <li> <p>Create a PBS Script for Finetuning    The following PBS script schedules a job to finetune Stable Diffusion using the <code>train_text_to_image_lora.py</code> script provided by the []<code>diffusers</code> github repository](https://github.com/huggingface/diffusers):    </p><pre><code># Clone the diffusers repository\ngit clone https://github.com/huggingface/diffusers\n</code></pre><p></p> </li> </ol> <pre><code>#!/bin/bash\n#PBS -l select=1:ncpus=4:ngpus=1:mem=16GB:gpu_model=V100\n#PBS -l walltime=2:00:00\n#PBS -j oe\n#PBS -N sd-finetune\n#PBS -M &lt;your_email&gt;@unsw.edu.au\n#PBS -m abe\n\n# Load necessary modules\nmodule load python\n\n# Set directories and variables\nexport base_dir=\"/srv/scratch/$USER/finetune\"\nexport model_name=\"CompVis/stable-diffusion-v1-4\"\nexport dataset_name=\"pranked03/flowers-blip-captions\"\n\n# Set Hugging Face cache to scratch\nexport HF_HOME=\"/srv/scratch/$USER/huggingface_cache\"\n\n# Activate the virtual environment\nsource $base_dir/venv_deforum/bin/activate\n\n# Finetune with the training script\naccelerate launch $base_dir/diffusers/examples/text_to_image/train_text_to_image_lora.py \\\n--pretrained_model_name_or_path=$model_name \\\n--dataset_name=$dataset_name \\\n--resolution=128 --center_crop --random_flip \\\n--train_batch_size=32 \\\n--gradient_accumulation_steps=1 \\\n--gradient_checkpointing \\\n--mixed_precision=\"fp16\" \\\n--max_train_steps=4000 \\\n--learning_rate=1e-05 \\\n--max_grad_norm=1 \\\n--lr_scheduler=\"constant\" --lr_warmup_steps=0 \\\n--output_dir=\"$base_dir/sd-flower-model\" \\\n--rank=8 \\\n--resume_from_checkpoint=\"latest\"\n</code></pre> <ol> <li> <p>Submit the Finetuning Job    Submit the job using:    </p><pre><code>qsub &lt;pbs_script_name&gt;.pbs\n</code></pre><p></p> </li> <li> <p>Monitor Progress    Monitor the progress of your training job using your HPC\u2019s job management commands, such as <code>qstat</code> or <code>squeue</code> (depending on the HPC system).</p> </li> </ol> <p>Important Reminder: Since the scratch directory is not backed up, it is essential to move any important files (e.g., your trained models) to your home directory after finetuning. You can use Katana OnDemand or following command to copy files from scratch to your home directory: </p><pre><code>cp -r /srv/scratch/$USER/finetune/sd-flower-model /home/$USER/\n</code></pre><p></p> <p>This ensures that your models are safely stored.</p>"},{"location":"AI_in_Katana/StableDiffusion/#video-tutorial-on-deforum","title":"Video Tutorial on Deforum","text":"<p>This video is a summary of the demo presenting the workflow of experimenting with Deforum on Katana</p>"},{"location":"help_support/faq/","title":"FAQ","text":""},{"location":"help_support/faq/#general-faq","title":"General FAQ","text":""},{"location":"help_support/faq/#where-is-the-best-place-to-store-my-code","title":"Where is the best place to store my code?","text":"<p>The best place to store source code is to use version control and store it in a repository.  This means that you will be able to  keep every version of your code and revert to an earlier version if you require. UNSW has a central github account,  which is best suited to research groups but you can also create your own repository.</p>"},{"location":"help_support/faq/#i-just-got-some-money-from-a-grant-what-can-i-spend-it-on","title":"I just got some money from a grant. What can I spend it on?","text":"<p>There are a number of different options for using research funding to improve your ability to run computationally intensive programs. The best starting point is to contact us to talk through the different options.  </p>"},{"location":"help_support/faq/#can-i-access-katana-from-outside-unsw","title":"Can I access Katana from outside UNSW?","text":"<p>Yes, if you have an account then you can connect to Katana from both inside and outside UNSW. Some services like Katana OnDemand are only available on campus so you will need to connect to the UNSW VPN. You may also notice that graphical connections are not as responsive when you are working remotely.</p>"},{"location":"help_support/faq/#account-faq","title":"Account FAQ","text":""},{"location":"help_support/faq/#how-can-i-request-an-account-on-katana","title":"How can I request an account on Katana?","text":"<p>To apply for an account, please send an email to restech.support@unsw.edu.au giving your zID, your role within UNSW and the name  of your supervisor or head of your research group.</p>"},{"location":"help_support/faq/#can-i-give-access-to-a-colleague-at-a-different-institution","title":"Can I give access to a colleague at a different institution?","text":"<p>Access to Katana requires a staff, student or other relationship with UNSW so access can only be granted if they have that relationship.</p>"},{"location":"help_support/faq/#when-will-my-katana-account-expire","title":"When will my Katana account expire?","text":"<p>Unless a request has been received to remove your access, your Katana account will remain open  whilst you have a student, staff or other relationship with UNSW. Once you no longer have that relationship your Katana account will be closed. </p>"},{"location":"help_support/faq/#what-happens-to-my-data-when-i-leave-the-university","title":"What happens to my data when I leave the university?","text":"<p>Before you leave the university you should upload any important data to the UNSW Data Archive. If you already have a Research Data Management Plan (RDMP)  then you can find upload instructions at www.dataarchive.unsw.edu.au where you will also find information on how to create a RDMP if you don't have one already.</p> <p>Once you leave UNSW your Katana account will be removed and data that you leave on Katana will be uploaded to the Data Archive in a project that only Restech staff have access to. Restech staff can provide a copy of your data to you if you return to UNSW or forgot  to take it when you left. Other people will only be able to access to your data if you give permission.  Data will be kept in the UNSW Data Archive for the following periods.</p> <ul> <li>Katana Home directories may be deleted after 5 years.</li> <li>Katana Scratch directories may be deleted after 5 years.</li> <li>Katana Shared Scratch directories may be deleted after 7 years.</li> <li>Katana configuration data (located in the setup directory) may be deleted after 7 years.</li> <li>Any misc. data that is not in an appropriate place may be deleted after 5 years.</li> </ul> <p>Once those times have been reached it will no longer be possible to retrieve a copy of your data.</p>"},{"location":"help_support/faq/#scheduler-faq","title":"Scheduler FAQ","text":""},{"location":"help_support/faq/#does-katana-run-a-32-bit-or-a-64-bit-operating-system","title":"Does Katana run a 32 bit or a 64 bit operating system?","text":"<p>Katana Compute Nodes and Head Node run a 64 bit version of the Rocky distribution of Linux. Currently version 8.9.</p>"},{"location":"help_support/faq/#how-much-memory-is-available-per-core-andor-per-node","title":"How much memory is available per core and/or per node?","text":"<p>The amount of memory available varies across the cluster. To determine how much memory each node has available use the 'pbsnodes' command. Roughly, you can safely use 4GB per core requested.  You can request more memory but it may delay time spent in the queue. Rather than requesting all of the memory on a compute node you should leave a few GB to allow room for the operating system to run and to ensure that you don't request too much memory for the node.</p>"},{"location":"help_support/faq/#how-much-memory-can-i-use-on-the-login-node-for-compiling-software","title":"How much memory can I use on the login node for compiling software?","text":"<p>The login nodes have a total of 24GB of memory each with each user limited to 4GB and should only be used to compile software or manage jobs. If you need more memory to compile your software then you do it in an Interactive Job.</p> <p>Note: If you compile software on a compute node then you should take care to ensure that your software is compatable with all of the nodes in Katana. The most common thing to be aware of is using CPU extensions like AVX which vary from node to node.</p>"},{"location":"help_support/faq/#why-isnt-my-job-making-it-onto-a-node-even-though-it-says-that-some-nodes-are-free","title":"Why isn't my job making it onto a node even though it says that some nodes are free?","text":"<p>There are three main reasons you will see this behavior. The first of them is specific to Katana and the other two apply to any cluster.</p> <p>Firstly, the compute nodes in Katana belong to various schools and research groups across UNSW. Any job with an expected run-time longer  than 12 hours can only run on a compute node that is somehow associated with the owner of the job. For example, if you are in the CCRC  you are entitled to run 12+ hour jobs on the General nodes and the nodes jointly purchased by CCRC. However, you cannot run 12+ hour  jobs on the nodes purchased by Astrobiology, Statistics, TARS, CEPAR or Physics. So you may see idle nodes, but you may not be  entitled to run a 12+ hour job on them.</p> <p>Secondly, the idle nodes may not have sufficient resources for your job. For example, if you have asked for 100GB memory but there is only 50GB free on the \"idle node\". You have requested 12 cpu cores but there are only 10 available. You may have requested a particular walltime and your job would not be finished before a bigger job that  will use at least some of these resources is due to begin. </p> <p>Thirdly, the most common example of resources waiting to be used by a job is with distributed memory jobs. They have a reservation on the node and they are just waiting for all of their  requested resources to become available. In this case, your job can only use the reserved nodes if your job can finish before the nodes are required by the  distributed memory job. For example, if a job has been waiting a week (yes, it happens) for <code>walltime=200,cpu=88,mem=600GB</code> (very long, two whole nodes), then those resources will need to be made available at some point. This is an excellent example of why breaking your jobs up into smaller parts is good practice.</p>"},{"location":"help_support/faq/#how-many-jobs-can-i-submit-at-the-one-time","title":"How many jobs can I submit at the one time?","text":"<p>Technically you can submit as many jobs as you wish. The queuing system run by the scheduler is designed to prevent a single user flooding the  system - each job will reduce the priority of your next jobs. In this way the infrequent users get a responsive system without impacting the regular users too much.</p> <p>Whilst there is not a technical limit to the number of jobs you can submit, submitting more that 1,000 jobs at the one time can place an unacceptable load on the  job scheduler and your jobs may be deleted without warning. This is an editorial decision by management.</p>"},{"location":"help_support/faq/#what-is-the-maximum-number-of-cpus-i-can-use-in-parallel","title":"What is the maximum number of CPUs I can use in parallel?","text":"<p>As many as your account and queue will allow you. But there are trade-offs -  if you ask for 150 CPUs (~5 full servers) you might be waiting more than a couple of months for your job to run. If your job is using multiple CPU cores amnd is still finishing in well under 12 hours you may wish to reduce the number of CPU cores meaning that your job will have more places to run (as long as it is still not requesting more than 12 hours).</p> <p>If you are regularly wanting to run large parallel jobs (over 32 cores per job) on Katana you should consider seeking support so that we are aware of your jobs.  We may be able to provide you additional assistance on resource usage for parallel jobs. </p>"},{"location":"help_support/faq/#why-does-my-ssh-connection-periodically-disconnect","title":"Why does my SSH connection periodically disconnect?","text":"<p>With all networks there is a limit to how long a connection between two computers will stay open if no data is travelling between them.  If you set your ServerAliveInterval or Keep Alive interval to 60 in your secure shell software (putty, ssh) then the connection will not be closed without warning. </p>"},{"location":"help_support/faq/#can-i-change-the-job-script-after-it-has-been-submitted","title":"Can I change the job script after it has been submitted?","text":"<p>Yes you increase the resource values for jobs that are still queued, but even then you are constrained by the limits of the particular queue  that you are submitting to. This means that if your job is in the 12 hour queue you cannot request a walltime of more than 12 hours. Once it has been assigned to a node the intricacies of the scheduling policy means that it becomes impossible for  anyone including the administrator to make any further changes.</p>"},{"location":"help_support/faq/#where-does-standard-output-stdout-go-when-a-job-is-run","title":"Where does Standard Output (STDOUT) go when a job is run?","text":"<p>By default Standard Output, that is the output you would normally see when you run your commands, is redirected to storage on the node and then transferred when the job is completed. If you are generating data you should redirect<code>STDOUT</code>  to a different location. The best location depends on the characteristics of your job but in general all<code>STDOUT</code> should be redirected to local scratch.</p>"},{"location":"help_support/faq/#how-do-i-figure-out-what-the-resource-requirements-of-my-job-are","title":"How do I figure out what the resource requirements of my job are?","text":"<p>The best way to determine the resource requirements of your job is to be generous with the resource requirements on the first run and then refine the requirements based on what the job actually used. We really don't mind you being generous with your resource requests whilst you are figuring out what your job needs. If you put the following information in your job script  you will receive an email when the job finishes which will include a summary of the resources used.</p> <pre><code>    #PBS -M z1234567@unsw.edu.au \n#PBS -m ae\n</code></pre>"},{"location":"help_support/faq/#can-i-cause-problems-to-other-users-if-i-request-too-many-resources-or-make-a-mistake-with-my-job-script","title":"Can I cause problems to other users if I request too many resources or make a mistake with my job script?","text":"<p>Yes, but it's extremely unlikely. We used to say no, but that's not strictly true. The reality is that if something breaks it's usually your job hitting the odd corner case we didn't account for. It doesn't happen often.</p>"},{"location":"help_support/faq/#will-a-job-script-from-another-cluster-work-on-katana","title":"Will a job script from another cluster work on Katana?","text":"<p>It depends on a number of factors including the sceduling software. Some aspects are fairly common across different clusters (e.g. walltime) others are not. You  should look at the cluster specific information to see what queuing system is being used on that cluster and what commands you will need to change. You won't  find a cluster that doesn't have knowledgable support that can help you migrate. It is also good to remember that the resources on the compute nodes will vary between  different clusters so you should confirm that your resource request is appropriate for Katana.</p>"},{"location":"help_support/faq/#how-can-i-see-exactly-what-resources-io-cpu-memory-and-scratch-my-job-is-currently-using","title":"How can I see exactly what resources (I/O, CPU, memory and scratch) my job is currently using?","text":"<p>From outside the job, you can run<code>qstat -f &lt;jobid&gt;</code>. </p> <p>If, for instance, you wanted to measure different steps of your process, then inside your jobscript you can put<code>qstat -f $PBS_JOBID</code></p> <p>For fine grain detail, you may need to get access to the worker node that the job is running on:</p> <pre><code>    qstat -nru $USER\n</code></pre> <p>then you can see a list of your running jobs and where they are running. You can then use ssh to log on to the individual nodes and run<code>top</code> or<code>htop</code> to see the load on the node including memory usage for each of the processes on the node.</p>"},{"location":"help_support/faq/#how-do-i-request-the-installation-or-upgrade-of-a-piece-of-software","title":"How do I request the installation or upgrade of a piece of software ?","text":"<p>You should first check to see if the software is already installed using the module command. If the software is not on the list and you wish to  have a new piece of software installed or software that is already installed upgraded the easiest way is to send an email to restech.support@unsw.edu.au from your UNSW email account with details of what software change you require.</p>"},{"location":"help_support/faq/#why-is-my-job-stuck-in-the-queue-whilst-other-jobs-run","title":"Why is my job stuck in the queue whilst other jobs run?","text":"<p>The queues are not set up to be first-in-first-out. In fact all of the queued jobs sit in one big pool of jobs that are ready to run. The scheduler assigns priorities to jobs in the pool  and the job with the highest priority is the next one to run. The length of time spent waiting in the pool is just one of several factors that are used to determine priority.</p> <p>For example, people who have used the cluster heavily over the last two weeks receive a negative contribution to their jobs' priority, whereas a user who hasn't used Katana much recently will receive a positive contribution. You can see this in action with the diagnose -p and diagnose -f commands.</p>"},{"location":"help_support/faq/#you-mentioned-waiting-time-as-a-factor-what-else-affects-the-job-priority","title":"You mentioned waiting time as a factor, what else affects the job priority?","text":"<p>The following three factors combine to generate the job priority.</p> <ul> <li>How many resources (cpu and memory) have you and your group consumed in the last 14 days? Your personal consumption is weighted more highly than your group's consumption.  Heavy recent usage contributes a negative priority. Light recent usage contributes a positive priority.</li> <li>How many resources does the job require? Always a positive contribution to priority, but increases linearly with the amount  of cpu and memory requested, i.e. we like big jobs.</li> <li>How long has the job been waiting in the queue? Always a positive contribution to priority, but increases linearly with the amount of time your job has been waiting  in the queue. Note that throttling policies will prevent some jobs from being considered for scheduling, in which case their clock does not start ticking until that  throttling constraint is lifted.</li> </ul>"},{"location":"help_support/faq/#what-happens-if-my-job-uses-more-memory-than-i-requested","title":"What happens if my job uses more memory than I requested?","text":"<p>The job will be killed by the scheduler. You will get a message to that effect if you have any types of notification enabled (logs, emails). If this happens you should increase the amount of memory that your job requests and resubmit your job.</p>"},{"location":"help_support/faq/#what-happens-if-my-job-is-still-running-when-it-reaches-the-end-of-the-time-that-i-have-requested","title":"What happens if my job is still running when it reaches the end of the time that I have requested?","text":"<p>When your job hits its Walltime it is automatically terminated by the scheduler.</p>"},{"location":"help_support/faq/#200-hours-is-not-long-enough-what-can-i-do","title":"200 hours is not long enough! What can I do?","text":"<p>If you find that your jobs take longer than the maximum WALL time then there are several different options to change your code so that it fits inside the parameters.</p> <ul> <li>Can your job be split into several independent jobs?</li> <li>Can you export the results to a file which can then be used as input for the next time the job is run?</li> </ul> <p>You may want to also look to see if there is anything that you can do to make your code run better like making better use of local scratch if your code is I/O intensive.</p>"},{"location":"help_support/faq/#do-sub-jobs-within-an-array-job-run-in-parallel-or-do-they-queue-up-serially","title":"Do sub-jobs within an array job run in parallel, or do they queue up serially?","text":"<p>Submitting an array job with 100 sub-jobs is equivalent to submitting 100 individual jobs. So if sufficient resources are available then all 100 sub-jobs could run in parallel.  Otherwise some sub-jobs will run and other sub-jobs must wait in the queue for resources to become available.</p> <p>The '%' option in the array request offers the ability to self impose a limit on the number of concurrently running sub-jobs. Also, if you need to impose an order on when  the jobs are run then the 'depend' attribute can help.</p>"},{"location":"help_support/faq/#in-a-pbs-file-does-the-mem-requested-refer-to-each-node-or-the-total-memory-on-all-nodes-being-used-if-i-am-using-more-than-1-node","title":"In a pbs file does the MEM requested refer to each node or the total memory on all nodes being used (if I am using more than 1 node?)","text":"<p>MEM refers to the amount of memory per node. If you are only requesting resources on a single node then this is the total amount of memory that you have requested.</p>"},{"location":"help_support/faq/#storage-faq","title":"Storage FAQ","text":""},{"location":"help_support/faq/#what-storage-is-available-to-me","title":"What storage is available to me?","text":"<p>Katana provides three different storage areas, cluster home drives, local scratch and global scratch. The storage page has additional information on the differences and advantages of each of the different types of storage. You may also want to consider storing your code using a  version control service like GitHub. This means that you will be able to keep every version of your code and revert to an earlier version if something goes wrong.</p>"},{"location":"help_support/faq/#which-storage-is-fastest","title":"Which storage is fastest?","text":"<p>In order of performance the best storage to use is local scratch, global scratch and cluster home drive.</p>"},{"location":"help_support/faq/#is-any-of-the-cluster-based-storage-backed-up","title":"Is any of the cluster based storage backed up?","text":"<p>The only cluster based storage that gets backed up is the cluster home drives. All files in local scratch are removed when your job completes. Files in global scratch are not backed up and you should keep a copy of any files that are important. You should also consider placing a copy of any important files in the UNSW Data Archive.</p>"},{"location":"help_support/faq/#how-do-i-actually-use-local-scratch","title":"How do I actually use local scratch?","text":"<p>Some software allows you to specify a temporary or working directory. If the software that you are ussing does not have that option then the easiest way of making use of local scratch is to use scripts to copy files to the node at the start of your job and from the node when your job finishes.</p>"},{"location":"help_support/faq/#why-am-i-having-trouble-creating-a-symbolic-link","title":"Why am I having trouble creating a symbolic link?","text":"<p>Not all filesystems support symbolic links. The most common examples are some Windows network shares. On Katana this includes Windows network shares such as hdrive. The target of the symbolic link can be within such a filesystem, but the link itself must be on a filesystem that supports symbolic links, e.g. the rest of your home directory or your scratch directory. </p>"},{"location":"help_support/faq/#what-storage-is-available-on-compute-nodes","title":"What storage is available on compute nodes?","text":"<p>Local scratch, global scratch and your cluster home drive are accessible on the compute nodes. You may be able to connect to your other storage via the Katana Data Mover and copy files to yyou global scratch dorectory.</p>"},{"location":"help_support/faq/#what-is-the-best-way-to-transfer-a-large-amount-of-data-onto-a-cluster","title":"What is the best way to transfer a large amount of data onto a cluster?","text":"<p>Use<code>rsync</code> to copy data to the KDM server. More information is above.</p>"},{"location":"help_support/faq/#is-there-any-way-of-connecting-my-own-file-storage-to-one-of-the-clusters","title":"Is there any way of connecting my own file storage to one of the clusters?","text":"<p>Whilst it is not possible to connect individual drives to any of the clusters, some units and research groups have purchased large capacity storage units which are co-located with the clusters. This storage is then available on the cluster nodes. For more information please contact the Research Technology  Services Team by sending an email to restech.support@unsw.edu.au.</p>"},{"location":"help_support/faq/#can-i-specify-how-much-file-storage-i-want-on-local-scratch","title":"Can I specify how much file storage I want on local scratch?","text":"<p>If you want to specify the minimum amount of space on the drive before your job will be assigned to a node then you can use the file option in your job script.  Unfortunately setting up more complicated file requirements is currently problematic.</p>"},{"location":"help_support/faq/#can-i-run-a-program-directly-from-scratch-or-my-home-drive-after-logging-in-to-the-cluster-rather-submitting-a-job","title":"Can I run a program directly from scratch or my home drive after logging in to the cluster rather submitting a job?","text":"<p>As the file server does not have any computational resources you would be running the job from the head node on the cluster. If you need to enter information when  running your job then you should start an interactive job.</p>"},{"location":"help_support/faq/#expanding-katana","title":"Expanding Katana","text":"<p>Katana has significant potential for further expansion. It offers a simple and cost-effective way for research groups to invest in a powerful computing facility and take advantage of the economies that come with joining a system with existing infrastructure. A sophisticated job scheduler ensures that users always receive a fair share of the compute resources that is at least commensurate with their research group\u2019s investment in the cluster. For more information please contact us.</p>"},{"location":"help_support/faq/#acknowledging-katana","title":"Acknowledging Katana","text":"<p>If you use Katana for calculations that result in a publication then you should add the following text to your work.</p> <p>This research includes computations using the computational cluster Katana supported by Research Technology Services at UNSW Sydney.</p> <p>Katana now also has a DOI that can be used for citation in papers: https://doi.org/10.26190/669x-a286</p> <p>If you are using nodes that have been purchased using an external funding source you should also acknowledge the source of those funds.</p> <p>For information refer to acknowledging ARC funding</p> <p>Your School or Research Group may also have policies for compute nodes that they have purchased.</p>"},{"location":"help_support/faq/#facilities-external-to-unsw","title":"Facilities external to UNSW","text":"<p>If you are using facilities at Intersect or NCI in addition to Katana they may also require some form of acknowledgement.</p> <p>Intersect NCI</p>"},{"location":"help_support/glossary/","title":"Glossary","text":""},{"location":"help_support/glossary/#active-job","title":"Active Job","text":"<p>Active jobs (also called running jobs) are jobs that have been assigned to a compute node and are currently running. These can be seen by running <code>qstat</code> and looking for a R in the second last column. See examples.</p>"},{"location":"help_support/glossary/#array-job","title":"Array Job","text":"<p>If you want to run the same job multiple times with slight differences (filenames, data sources, variables, etc), then you can create an array job which will submit multiple jobs for you from the one job script. </p>"},{"location":"help_support/glossary/#batch-job","title":"Batch Job","text":"<p>A batch job is a job on a cluster that runs without any further input once it has been submitted and will start running as soon as it reaches a compute node even if it is the middle of the night. Almost all jobs on the cluster are batch jobs with the remainder being Interactive Jobs including those that are run  using Katana OnDemand. See examples.</p>"},{"location":"help_support/glossary/#cluster","title":"Cluster","text":"<p>A computational cluster is a set of connected computers that work together to create a single system. They are often referred to as supercomputers or HPC (High Performance Computing) systems. All clusters will have Management Servers,  Login Nodes and Compute Nodes. Most clusters also have one of more Data Transfer Nodes. The data transfer node for Katana is called the Katana Data Mover or KDM.</p>"},{"location":"help_support/glossary/#compute-nodes","title":"Compute Nodes","text":"<p>The compute nodes are where the compute jobs run. Users submit jobs from the Login Node and the Job Scheduler on the Head Node will assign the job to one or more compute nodes.</p>"},{"location":"help_support/glossary/#cpu-core","title":"CPU Core","text":"<p>Each node in the cluster has one or more CPUs each of which has 6 or more cores. Each core is able to run one  job at a time so a node with 12 cores could have 12 jobs running in parallel.</p>"},{"location":"help_support/glossary/#data-transfer-node","title":"Data Transfer Node","text":"<p>The Data Transfer Node, also known as the Katana Data Mover (KDM), is a server that is used for transferring files to, from, and within the cluster. Due to the nature of moving data around, it uses a significant amount of memory and network bandwidth. This server is used to take that load off the Login Node.</p>"},{"location":"help_support/glossary/#environment-variable","title":"Environment Variable","text":"<p>Environment variables are variables that are set in Linux to give programs information. These all start with a $ symbol and could be something like your usename or the computer name, or where to find programs and set options. They will start with a $ symbol. </p> <p>For example, all users can reference <code>$TMPDIR</code> in their Job Script in order to use Local Scratch</p>"},{"location":"help_support/glossary/#global-scratch","title":"Global Scratch","text":"<p>Global scratch is a large data store that isn't backed up. It is availble from all nodes in Katana including the from every node including the Head NodeKatana Data Mover (KDM). If you have data files  or or want to save your computation results when your job finishes this is where you should put them.</p>"},{"location":"help_support/glossary/#head-node","title":"Head Node","text":"<p>The head node of the Cluster is the computer that manages job and resource management. This is where the  Job Scheduler and Resource Manager are run. It is kept separate from  the Login Node so that the cluster can keep running if something goes wrong with the Login Node. It also means that if anything goes wrong with the scheduler it doesn't affect the users who are using the login nodes.</p>"},{"location":"help_support/glossary/#held-jobs","title":"Held Jobs","text":"<p>Held jobs are jobs that cannot currently run. They are most often put into that state by either the server or the system administrator but users an also put their jobs on hold. Users sometime want to submit multiple jobs at the same time but want to control the order in which they run. Unless a job has been held by a user, it will stay on the held state until released by one of the people managing Katana, at which point they become Queued Jobs. These can be seen by running <code>qstat</code> and looking for an H in the second last column. See examples.</p>"},{"location":"help_support/glossary/#interactive-job","title":"Interactive Job","text":"<p>An interactive job is a way of running your software on a cluster without negatively impacting the Login Node.  Once a request has been submitted and starts running on a Compute Nodes with the resources that you requestd. You can thn figure out exactly what you need to do to run your program and you can use that information to convert your commands into a baatch job. All jobs on Katana are either a Batch Job or an interactive job. Instructions on using interactive jobs.</p>"},{"location":"help_support/glossary/#job-scheduler","title":"Job Scheduler","text":"<p>The job scheduler monitors the jobs currenty running on the cluster and assigns Queued Jobs to Compute Nodes based on recent cluster useage, job resource requirements and nodes available to the research group of the submitter. To put it another way, the job scheduler determines when and where a job should run. The job scheduler that is used on Katana is called Owe use is called OpenPBS.</p>"},{"location":"help_support/glossary/#job-script","title":"Job Script","text":"<p>A job script is a file containing all of the information needed to run a Batch Job including the resource requirements and the commands to run the job. A job script can be thought of as a file containing everything that you would need to type in if you wre running an interative job. See examples.</p>"},{"location":"help_support/glossary/#local-scratch","title":"Local Scratch","text":"<p>Local scratch refers to the storage available internally on each compute node. Of all the different storage options on Katana storage has the best performance which can improve the performance of your job especially if it generates a large number of small files. The downside of using local scratch is that you will need to copy your data in when your job starts and then copy it out when your job finishes as local scratch is removed when your job ends. If the software that you are using allows you to specify a working or temp directory then you can just specify local scratch as the option. Otherwise you will need to add something to  your job script to move data in and out. You can use local scratch with the Environment Variable <code>$TMPDIR</code>.</p>"},{"location":"help_support/glossary/#login-node","title":"Login Node","text":"<p>The login nodes of the cluster is the computer that you connect to when you log in to the cluster. This node is most  often used to manage jobs incuding checking their status but is also used to compile software. </p>"},{"location":"help_support/glossary/#module","title":"Module","text":"<p>The module command is a means of providing access to software including allowing you to chose between different versions of same software. Read more on the Environmental Modules page.</p>"},{"location":"help_support/glossary/#management-servers","title":"Management Servers","text":"<p>The Management Servers are a collection of servers, including the Head Node, that can only be accessed by the people running the cluster. These servers are used to create accounts, schedule jobs, provide computational software, manage Katana storage and keep Katana running.</p>"},{"location":"help_support/glossary/#mpi","title":"MPI","text":"<p>Message Passing Infrastructure (MPI) is a technology for running a Batch Job on more than one Compute Node. Some software has been designed for situations where parts of the job can run on independent compute nodes with the results being transferred to other nodes for the next part of the job to be run.</p>"},{"location":"help_support/glossary/#nci","title":"NCI","text":"<p>NCI is a national supercomputing facility in Canberra. It is home to Gadi, one of the most powerful supercomputers in the world. Every year, hundreds of UNSW staff and postgraduate students use Gadi to support their research. More information is available on the Restech site.</p>"},{"location":"help_support/glossary/#network-drive","title":"Network Drive","text":"<p>A network drive is a drive that is independent from Katana. Whilst files are not accessible from the login or compute nodes it may be possible to get access to your files on Katana Data Mover (KDM) and copy files across.</p>"},{"location":"help_support/glossary/#queue","title":"Queue","text":"<p>All submitted jobs are put into a queue which has a collection of resources available to it. As jobs finish and those resources  become available, new jobs will be assigned to those resources. Figuring out which job will run next is done by the scheduler and depends on a number of factors including length of wait time and total resource use by the user over the previous month.</p>"},{"location":"help_support/glossary/#queued-jobs","title":"Queued Jobs","text":"<p>Queued jobs are jobs that are waiting for a Compute Nodes that matches their requirements to become available.  Which idle job will be assigned to a compute node next depends on the Job Scheduler and can be seen by  running <code>qstat</code> and looking for a Q in the second last column. See examples.</p>"},{"location":"help_support/glossary/#resource-manager","title":"Resource Manager","text":"<p>A resource manager works with the Job Scheduler to manage running jobs on a cluster. Amongst other tasks it receives and parses job submissions, starts jobs on Compute Nodes, monitors jobs, kills jobs, and manages  how many CPU Core are available on each Compute Nodes</p>"},{"location":"help_support/glossary/#scratch-space","title":"Scratch Space","text":"<p>Scratch space is a non backed up storage area where users can store transient data. It should not be used for job code as it is not backed up.</p>"},{"location":"help_support/glossary/#walltime","title":"Walltime","text":"<p>Om HPC systems like Katana, walltime is the amount of time that you will be allocated when your job runs. If your job runs longer than the walltime,  it will be killed by the Job Scheduler to free up resources for one of the waiting jobs. Walltiime is one of the factors used by the scheduler to decide when your job will run.  On Katana Walltime is also used to determine which Queue your job will be assigned to. The shorter the walltime, the more  opportunity your job has to run which in turn means that it will start sooner. In short, running a job with a walltime of 12 hours is easier than running a job will a walltime of 100 hours. It is imprtant to note rhat youur job will be scheduled based on the walltime that you have requested and not how long your job takes to run.</p>"},{"location":"help_support/reference_data/","title":"Reference Data","text":""},{"location":"help_support/reference_data/#reference-data","title":"Reference Data","text":"<p>We keep a number of reference data sets available on Katana at <code>/data/</code> so that we don't accidentally - for instance - end up with 150 copies of the Human Genome in user's home directories.</p> <p>As these are reference data, they don't change often and we can update them as necessary.</p> Directory Description Update Schedule URL annovar Reference datasets that come with software installation. Installed when software is installed. annovar.openbioinformatics.org antismash Reference files and commands for antismash version 4.2.0 Version specific database installed when software is installed antismash.secondarymetabolites.org blast NCBI nr, nt, refseq_genomic and refseq datasets Updated on release 6 times a year www.ncbi.nlm.nih.gov/refseq blastv5 Version 5 of NCBI nr, nt, refseq_genomic and refseq datasets.  Updated on release 6 times a year. www.ncbi.nlm.nih.gov/refseq diamond Diamond reference databases for versions 0.8.38, 0.9.10, 0.9.22 and 0.9.24. Database format periodically changes. Updated when NCBI nr databases are updated. ab.inf.uni-tuebingen.de/software/diamond gtdbtk Version specific database installed when software is installed. gtex Genotype-Tissue Expression project, comprehensive resource to study tissue-specific gene expression and regulation Please contact the [Oates lab](mailto:e.oates@unsw.edu.au) for access to a large set of GTEx datathat is not publicly available. https://gtexportal.org/home/ hapcol Reference datasets that come with software installation. Installed when software is installed. hapcol.algolab.eu hg19 Human reference genome hg19 (GRCh37). Fixed reference. Never updated. www.ncbi.nlm.nih.gov/grc interproscan Reference datasets for InterProScan versions 5.20-59.0 and 5.35-74.0 Version specific database installed when software is installed. www.ebi.ac.uk/interpro itasser Rererence datasets for I-TASSER plus link to current nr database. Version specific databases installed when software is installed plus link to nr database (see blast above). zhanglab.ccmb.med.umich.edu/I-TASSER kaiju Reference databases for all versions of Kaiju. Same databases for all versions. Databases installed when software is installed. kaiju.binf.ku.dk matam Reference databases for all MATAM versions. Version specific database installed when software is installed. github.com/bonsai-team/matam megan Reference databases for all MEGAN versions. Version specific database installed when software is installed. ab.inf.uni-tuebingen.de/software/megan6 repeatmasker Reference datasets for RepeatMasker version 4.0.7 Version specific database installed when software is installed. www.repeatmasker.org sra Sequence Read Archive, repository of high throughput sequencing data https://www.ncbi.nlm.nih.gov/sra trinotate Reference databases for all versions of Kaiju. Same databases for all versions. Databases installed when software is installed. trinotate.github.io"},{"location":"help_support/user_support/","title":"Help and Support","text":""},{"location":"help_support/user_support/#help-and-support","title":"Help and Support","text":""},{"location":"help_support/user_support/#contact-the-research-technology-services-team","title":"Contact the Research Technology Services team","text":"<ul> <li> <p>Email restech.support@unsw.edu.au for general Katana issues including functional issues, software installation and reference data sets. </p> </li> <li> <p>Casual message via Microsoft Teams Drop-In Forum to get advice from peers or connect with other researchers. Our technical staff are also active in this Teams group. Information on how to join can be found here</p> </li> <li> <p>Data and storage For questions about research data at UNSW - on  storage, movement or Data Management Plans, please email the Research Data Team. If you wish to increase  your Katana storage, please email restech.support@unsw.edu.au - note that such increases are not automatic.</p> </li> </ul> <p>Info</p> <p>This is the best way to get help from UNSW Research Technology Services. You must use your UNSW email address or your zID. Without this information, we don't know who you are and may be unable to solve your issue.</p> <p>When writing your email, please include a clear and detailed description of the request or issue experienced including any error messages and node name if appropriate.  Something like \"It doesn't work\" doesn't help us help you! If at all possible, include the steps someone else needs to do to reproduce the problem, the job identifier, the date and time of your problem and on which Katana node it occurred, the script filename and the directory you were running from.</p> <p>Example of a bad request     I'm trying to do some work on katana, but it seems to be running slowly today.</p> <p>Example of a great request     When I tried to run Sentaurus TCAD today (2020-05-01) on Katana I got this error message regardless of structures I wanted to simulate:</p> <pre><code>Job failed\nError: Child process with pid '116643' got the signal 'SIGSEGV' (segmentation violation)\njob exits with status 1\n\nMy job ran on k052 with jobid 300000, my zID is z2134567\n</code></pre>"},{"location":"help_support/user_support/#katana-system-status-and-known-issues","title":"Katana System Status and Known Issues","text":"<p>No known issues at the moment.</p>"},{"location":"imaging/neurodesk/","title":"Neurodesk","text":""},{"location":"imaging/neurodesk/#neurodesk","title":"Neurodesk","text":"<p>Neurodesk is a powerful platform designed to make analyzing medical imaging data easier and more reliable. It uses special software packages called containers, which help ensure that your analysis is consistent and reproducible. With Neurodesk, you can easily set up and use these containers to work with your neuroimaging data, making the whole process smoother and more efficient.</p> <p> </p>"},{"location":"imaging/neurodesk/#jupyter-interactive-notebooks","title":"Jupyter interactive notebooks","text":"<p>Open up a terminal to execute the following commands.</p>"},{"location":"imaging/neurodesk/#set-up-your-python-environment-and-corresponding-jupyter-kernel","title":"Set up your python environment and corresponding jupyter kernel","text":"<pre><code> # Create and activate environment\nmodule load python/3.11.3\n python3 -m venv /home/$USER/.venvs/neurodesk\n source /home/$USER/.venvs/neurodesk/bin/activate\n\n # Install libraries to set up jupyter kernel\npip install --upgrade pip setuptools\n pip install seaborn numpy nibabel pandas matplotlib ipyniivue\n pip install ipykernel IPython==8.22.2\n deactivate\n\n # Import desired Neurodesk modules on Katana to your virtual environment\nmodule purge\n module load neurodesk/use\n # Here is where you'll load your chosen Neurodesk modules\nmodule load neurodesk/qsmxt/7.2.2\n module load neurodesk/mrtrix3/3.0.3\n module load python/3.11.3\n\n # Convert virtual environment to jupyter kernel\nsource /home/$USER/.venvs/neurodesk/bin/activate\n jupyter_kernel_from_env neurodesk\n</code></pre> <p>Note</p> <p>To install Neurodesk containers as modules to be used in Katana, please contact our service desk at restech.support@unsw.edu.au from your UNSW email account with details of what software change you require.</p>"},{"location":"imaging/neurodesk/#start-a-jupyterlab-instance-on-katana","title":"Start a JupyterLab instance on Katana","text":"<ol> <li>Log into Katana OnDemand and select the JuypterLab.     </li> <li>Select JupyterLab from the list of applications and choose the desired Compute Node and Walltime.     </li> <li>Click Launch to start the JupyterLab instance.</li> <li>Once the instance is running, click on the Launch JupyterLab button to open the JupyterLab interface in a new tab.     </li> <li>In the JupyterLab interface, select the Kernel menu and choose the kernel you created in step 1 (e.g., <code>neurodesk</code>).     </li> <li>You can now create a new notebook or open an existing one and then selecting the kernel to start working with the Neurodesk environment. An example notebook with QSMxT can be downloaded from the Neurodesk website.</li> </ol>"},{"location":"imaging/neurodesk/#running-an-example-jupyter-notebook-from-neurodesk","title":"Running an example Jupyter notebook from Neurodesk","text":"<ol> <li>Download the example notebook from the Neurodesk website: QSMxT Example Notebook.</li> <li>Upload the notebook to your JupyterLab instance. Instructions can be found in the Storage section.</li> <li>Modify the notebook to work with Katana specifically:<ul> <li>Comment out the lmod commands, as the modules are already loaded in the environment from Katana modules.  <pre><code># import lmod\n# await lmod.load('qsmxt/7.2.2')\n</code></pre></li> <li>Change all instances of AnyNiivue import to NiiVue <pre><code># from ipyniivue import AnyNiivue\nfrom ipyniivue import NiiVue\n# nv_T1 = AnyNiivue()\nnv_T1 = NiiVue()\n</code></pre></li> </ul> </li> </ol>"},{"location":"software/environment_modules/","title":"Environment Modules","text":"<p>Environment Modules are a means of changing your environment to run specfic versions of installed sotware. This section provides information on how they are used on Katana.</p>"},{"location":"software/environment_modules/#how-do-i-discover-what-software-is-available","title":"How do I discover what software is available?","text":"<pre><code>[z1234567@katana ~]$ module avail \n\n-------------------------- /share/apps/modules/intel ---------------------------\nintel/11.1.080(default)  intel/12.1.7.367  intel/13.0.1.117  intel/13.1.0.146\n\n--------------------------- /share/apps/modules/pgi ----------------------------\npgi/13.7\n\n-------------------------- /share/apps/modules/matlab --------------------------\nmatlab/2007b          matlab/2010b          matlab/2012a(default)\nmatlab/2008b          matlab/2011a          matlab/2012b\nmatlab/2009b          matlab/2011b          matlab/2013a\n</code></pre>"},{"location":"software/environment_modules/#what-if-the-software-that-i-want-is-not-on-the-list","title":"What if the software that I want is not on the list?","text":"<p>If you require additional software installed on Katana then please email the details to restech.support@unsw.edu.au including URLs and the version number that you wish installed.</p>"},{"location":"software/environment_modules/#how-do-i-add-a-particular-version-of-software-to-my-environment","title":"How do I add a particular version of software to my environment?","text":"<pre><code>[z1234567@katana1 ~]$ module add matlab/2018b\n</code></pre> or <pre><code>[z1234567@katana1 ~]$ module load matlab/2018b\n</code></pre>"},{"location":"software/environment_modules/#which-versions-of-software-am-i-currently-using","title":"Which versions of software am I currently using?","text":"<pre><code>[z1234567@katana1 ~]$ module list\n    Currently Loaded Modulefiles:\n     1) intel/18.0.1.163   2) matlab/2018b\n</code></pre>"},{"location":"software/environment_modules/#how-do-i-remove-a-particular-version-of-software-from-my-environment","title":"How do I remove a particular version of software from my environment?","text":"<pre><code>[z1234567@katana1 ~]$ module rm matlab/2018b\n</code></pre> <p>or</p> <pre><code>[z1234567@katana1 ~]$ module unload matlab/2018b\n</code></pre>"},{"location":"software/environment_modules/#how-do-i-remove-all-modules-from-my-environment","title":"How do I remove all modules from my environment?","text":"<pre><code>[z1234567@katana1 ~]$ module purge\n</code></pre>"},{"location":"software/environment_modules/#how-do-i-find-out-more-about-a-particular-piece-of-software","title":"How do I find out more about a particular piece of software?","text":"<p>You can find out more about a piece of software by using the module help command. For example:</p> <pre><code>[z1234567@katana1 ~]$ module help mrbayes\n\n----------- Module Specific Help for 'mrbayes/3.2.2' --------------\n\nMrBayes 3.2.2 is installed in /apps/mrbayes/3.2.2\n\nThis module was complied against beagle/2.1.2 and openmpi/1.6.4 with MPI support.\n\nMore information about the commands made available by this module is available\nat http://mrbayes.sourceforge.net\n</code></pre>"},{"location":"software/environment_modules/#how-do-i-switch-between-particular-versions-of-software","title":"How do I switch between particular versions of software?","text":"<pre><code>[z1234567@katana1 ~]$ module switch matlab/2018b matlab/2017b\n</code></pre>"},{"location":"software/environment_modules/#how-can-i-find-out-what-paths-and-other-environment-variables-a-module-uses","title":"How can I find out what paths and other environment variables a module uses?","text":"<pre><code>[z1234567@katana1 ~]$ module show mothur/1.42.3\n-------------------------------------------------------------------\n/apps/modules/bio/mothur/1.42.3:\n\nmodule-whatis     Mothur 1.42.3 \nconflict     mothur \nsetenv         MOTHUR_ROOT /apps/mothur/1.42.3 \nprepend-path     PATH /apps/mothur/1.42.3/bin \nsetenv         LAST_MODULE_TYPE bio \nsetenv         LAST_MODULE_NAME mothur/1.42.3 \nsetenv         LAST_MODULE_VERSION 1.42.3 \n-------------------------------------------------------------------\n</code></pre>"},{"location":"software/environment_modules/#why-does-the-cluster-forget-my-choice-of-modules","title":"Why does the cluster forget my choice of modules?","text":"<p>Environment modules are desined to make a temporary change to your evironment and only affect the session in which they are loaded. Loading a module in one SSH session  will not affect any other SSH session or any jobs submitted from that session. Modules must be loaded in every session where they will be used.</p>"},{"location":"software/environment_modules/#how-can-i-invoke-my-module-commands-automatically","title":"How can I invoke my module commands automatically?","text":"<p>Add environment module commands to your job script. This approach is useful for preserving the required environment for each job. For example:</p> <pre><code>#!/bin/bash\n\n#PBS -l select=1:ncpus=1:mem=4gb\n#PBS -j oe\n\nmodule purge\nmodule add intel/18.0.1.163\n\ncd ${PBS_O_WORKDIR}\n\n./myprog\n</code></pre>"},{"location":"software/environment_modules/#how-do-module-files-interact-with-perl-python-and-r","title":"How do module files interact with Perl, Python and R?","text":"<p>Perl, Python and R all have their own library/module systems: CPAN, PyPI and CRAN. Information on  installing a library or module for yourself can be found on the help pages for Perl, Python and R. If you are having difficulty installing the library or module then can help you.  Please email such requests to restech.support@unsw.edu.au.</p>"},{"location":"software/installing_software/","title":"Installing software","text":""},{"location":"software/installing_software/#checking-if-a-package-is-installed","title":"Checking if a package is installed","text":"<p>Note</p> <p>Before installing software yourself, check if it has already been installed as part of the environment or as a module. You may also want to speak to one of your research colleagues to find out if they have installed it for themselves.</p> <p>Some common software packages are already available as part of katana's the operating system. They are listed by: </p> <pre><code>    yum list installed\n</code></pre> <p>Warning</p> <p>Do not try to run administration commands as a user. These include: apt-get install, yum install, su, or sudo. </p> <p>You can see the list of software applications already installed on Katana using the Environment Module command:</p> <pre><code>   module avail\n</code></pre>"},{"location":"software/installing_software/#r-and-python-packages","title":"R and Python Packages","text":"<p>Many Python and R packages are installed on katana. We have specific documentation to install your own packages in Python` and R</p> <p>Note</p> <p>If you have tried to install packages software yourself, but you need further assistance, then please send an email to restech.support@unsw.edu.au.</p>"},{"location":"software/installing_software/#installing-a-binary-package","title":"Installing a binary package","text":"<p>While installing from source is preferred for effeciency reasons, sometimes only precompiled binaries are available. When downloading binaries, make sure you select the correct architecture and operating system for Katana.</p> <p>Note</p> <p>The login and compute nodes of Katana are currently Rocky Linux 8.9, 64-bit, Intel x86_64 / AMD64.</p> <p>If the software is small then you may want to install it in your home directory to take advantage of the nightly backup. Otherwise you should install the software in your scratch directory.</p> <pre><code>wget https://website.org/binary/application  </code></pre> <p>Change file permissions to make the application readable and executable</p> <pre><code>chmod u+rx ./application\n</code></pre>"},{"location":"software/installing_software/#compiling-from-source","title":"Compiling from source","text":"<p>Compiling from source is preferred for efficiency reasons but is generally a more complicted process than a binary installation. </p> <p>Note</p> <p>If the software is small then you may want to install it in your home directory to take advantage of the nightly backup. Otherwise you should install the software in your scratch directory.</p>"},{"location":"software/installing_software/#installing-software-from-github","title":"Installing software from Github","text":"<p>Source code is commonly stored on GitHub for easy version control. Git is available by default on katana. Remember: UNSW has its own GitHub organisation.</p> <p>The process to install code which comes from GitHub depends on how the author of the code has set it up. </p>"},{"location":"software/installing_software/#installing-a-github-software-release","title":"Installing a Github software release","text":"<p>Often the software owner will create a software release  which is a copy of the software with everything frozen at that point in time. This helps with reproducability of results as you can refer to a specific version of the software and someone else can easily install the same version on their computer. You will often be able to choose between a binary version and downloading the source. Downloading the binary and the source versions will look something like:</p> <pre><code>   wget https://github.com/project/project/releases/download/v1.48.1/project.linux_8.zip\n</code></pre> <pre><code>   wget https://github.com/project/project/archive/refs/tags/v1.48.1.tar.gz\n</code></pre>"},{"location":"software/installing_software/#github-cloning","title":"Github cloning","text":"<p>If the owner of the software has not created a release or the latest release is too old then you can download the repository and use it to compile the software. Copy the web address revealed by the green 'Code' button on the repository. Creating a local copy of the repository uses the following command:</p> <pre><code>   git clone https://github.com/project/project.git\n</code></pre> <p>The created folder will then contain the source code and some documentation files.</p>"},{"location":"software/installing_software/#readme-and-install-files","title":"README and INSTALL files","text":"<p>The README file contains general information for the software, and often a brief installation guide. INSTALL will contain more detailed installation instructions, including configuration for  certain archictures. Please read the README and INSTALL files in full before attempting compilation.</p>"},{"location":"software/installing_software/#compilers","title":"Compilers","text":"<p>It is generally best to use the system compilers <code>gcc</code> and <code>ld</code>. However, many code requires specific compilers and versions. Katana has many compilers available as modules  including the Intel Compilers and Software Libraries</p> <p>Note</p> <p>Please install software using an interactive session, qsub -I, not directly on the login node. </p>"},{"location":"software/installing_software/#configuring-installation-files","title":"Configuring installation files","text":"<p>Commonly, a configuration script is available which allows you to set where the software is installed by using the --prefix flag as well as other options. To install the software in your Katana home directory you can use the following command:</p> <pre><code>   ./configure --prefix=$HOME/apps/{PACKAGE}/{VERSION}\n</code></pre> <p>The software can then be installed according to the rules in the MakeFile. This is typically invoked with</p> <pre><code>   make\n   make install\n</code></pre>"},{"location":"software/installing_software/#creating-module-files","title":"Creating module files","text":"<p>Much like katana's <code>Environment Modules</code>, you can also have multiple versions of the application available through your own modules.</p> <p>The template for environment modules is in:</p> <pre><code>/apps/modules/templates/module_file\n</code></pre> <p>The template module file makes some assumptions and examples, which may not be applicable to your software. Key sections will likely need to modify are:</p> <ul> <li>set      basepath          $env(HOME)/apps/{SOFTWARE_NAME}</li> <li> <p>set      version           {VERSION_NUMBER}        </p> </li> <li> <p>set      url</p> </li> <li> <p>set      installed</p> </li> <li> <p>set      compiled_with</p> </li> <li> <p>set      mpiversion</p> </li> <li> <p>prereq     {PREREQUISITE_SOFTWARE)</p> </li> </ul> <p>Note</p> <p>Insert \"module use --append $HOME/apps/Modules\" into your ~/.bashrc to enable using your own modules upon login. </p> <p>You should be able to module load your own software module as your own.</p>"},{"location":"software/jupyter-notebooks/","title":"Jupyter Notebooks","text":"<p>Jupyter Notebooks and JupyterLab are best run via Katana OnDemand. </p> <p>When you use Jupyter Notebooks and JupyterLab on Katana OnDemand it comes with some built in environments and kernels  that are available for use. If none of these work for you then you can install your own environment or kernel.</p>"},{"location":"software/jupyter-notebooks/#python-virtual-environments","title":"Python virtual environments","text":"<p>If you need to use the Jupyter Notebook or Jupyter Lab with your own  Python Virtual Environments you will need to create your own Python Jupyter kernel using the instructions below:</p>"},{"location":"software/jupyter-notebooks/#create-and-load-the-virtual-environment","title":"Create and load the virtual environment","text":"<pre><code>$ module load python/3.8.3\n$ python3 -m venv --system-site-packages /home/z1234567/.venvs/jupyter-kernel\n$ source /home/z1234567/.venvs/jupyter-kernel/bin/activate\n$ pip install ipykernel\n</code></pre> <p>Create the Jupyter Kernel </p> Using the helper scriptInstalling the kernel manually <p>This script can automatically setup Jupyter kernels for use in Katana OnDemand.</p> <pre><code>(jupyter-kernel) $ install_jupyter_kernels\n</code></pre> <pre><code>(jupyter-kernel) $ python3 -m ipykernel install --prefix=$HOME/.local --name=jlab-kernel\n</code></pre> <p>Warning</p> <p>The <code>--name=XXXX</code> isn't strictly necessary, but if you don't use it,  the kernel will be called \"Python 3\". This will not be distinguishable from the Katana supplied Python 3 and could cause confusion.</p> <p>Now when you load a JupyterLab session, you should see your personal kernel  in the list of available kernels. This kernel will have access to your virtual environment.</p>"},{"location":"software/jupyter-notebooks/#conda-environments","title":"Conda environments","text":"<p>If you need to use the Jupyter Notebook or Jupyter Lab with your own Conda environment you will need to create your own Python or R Jupyter kernel. Here is an example:</p>"},{"location":"software/jupyter-notebooks/#create-and-activate-the-environment","title":"Create and activate the environment","text":"<pre><code>$ conda create -n my_conda_env -c conda-forge ipykernel\n$ conda activate my_conda_env\n</code></pre> Once you have activated your Conda environment you will need to create your Jupyter kernal using one of the commands listed above."},{"location":"software/jupyter-notebooks/#other-jupyter-kernels","title":"Other Jupyter Kernels","text":"<p>Kernels other than <code>ipykernel</code>, need <code>jupyter</code> to be installed in addition to the kernel.</p> <p>For example, to use the R kernel <code>r-irkernel</code>, you must also install the <code>jupyter</code> package before running the helper script.</p> <pre><code>(my_conda_env) $ conda install jupyter\n</code></pre>"},{"location":"software/operating_systems/","title":"Operating Systems","text":"<p>Katana nodes currently run Rocky Linux 8.9.</p> <p>Research software is installed in environment modules. This enables multiple versions of the same software to be installed, and each user can choose which version they wish to use.</p>"},{"location":"software/others/","title":"Others","text":""},{"location":"software/others/#ansys","title":"Ansys","text":"<p>Both Ansys Workbench and Ansys Electronic Desktop are available on Katana. The most user friendly way to run Ansys is to use Katana OnDemand. </p> <p>Ansys Batch Jobs</p> <p>Note: The version of Ansys available via myAccess MUST NOT be used for any research including generating files to be used on Katana.</p> <p>Once you are familiar with how running jobs on Katana works you can run them in a batch job which mean that your jobs don't need any input from you. </p> <p>Ansys CFX</p> <p>If you want to use Ansys CFX on Katana then you will need to upload the .cfx and .def files to Katana.</p> <p>A brief batch script example is given below. For more detail visit our GitHub page.  (Note: You need to join the UNSW GitHub organisation to access this repository)</p> <pre><code>   #!/bin/bash\n#&lt;RESOURCE REQUESTS&gt;\n\ncd $PBS_O_WORKDIR\n\nmodule load intel-mpi/2021.7.1\n   module load ansys/2021r1\n\n   cfx5solve -batch -def &lt;filename&gt;.def -part $NCPUS -start-method \"Intel MPI Local Parallel\"\n</code></pre> <p>Ansys Fluent</p> <p>Ansys Fluent input can also be generated locally and transferred to Katana to run in a batch job with a brief bash script shown here snd more complete examples in our Github repository. (Note: You will need to  join the UNSW GitHub organisation to access this repo).</p> <pre><code>   #!/bin/bash\n#&lt;RESOURCE REQUESTS&gt;\n\ncd $PBS_O_WORKDIR\n\nmodule load ansys/2021r1\n   module load intel-mpi/2021.7.1\n\n   fluent 3d -g -t $NCPUS -i fluent.in &gt; output.out\n</code></pre> <p>Ansys on Gadi</p> <p>If you have an account on Gadi, the Supercomputer located at NCI, you can also use Ansys there once you join the on once you have an account. UNSW has a institutional licence that requies you join the relevant software group as detailed in NCI's documentation.</p>"},{"location":"software/others/#biosciences","title":"Biosciences","text":"<p>There are a number of Bioscience software packages installed. If you are having trouble finding a software package it might be within another package such as the ones mentioned on this page.</p> <p>Datasets</p> <p>To avoid multiple users needing to download datasets for themselves we have downloaded some of the most commonly used ones, such as multiple versions of the NCBI <code>nr</code> and <code>nt</code> databases, into the <code>/data</code> directory. </p> <p>Stand alone</p> <p>Blast+: <code>module load blast-plus/2.12.0</code></p> <p>Mothur: <code>module load mothur/1.48.0</code></p> <p>Python Module</p> <p>Any of the Python versions that you see when running the module command on Katana will include BioPython and Snakemake. <code>module avail python</code></p> <p>R Module</p> <p><code>module avail r</code></p> <p>As Bioconductor is installed within R the best approach is to load the latest version of R to reduce the possibility of dependency issues. Then you can install it in your personal R library by following the instructions at the Bioconductor web site. An example of how to install Bioconductor and some Bioconductor packages is shown below.</p> <pre><code>    &gt; if (!require(\"BiocManager\", quietly = TRUE))\ninstall.packages(\"BiocManager\")\n&gt; BiocManager::install(\"Biobase\")\n&gt; BiocManager::install(c(\"GenomicFeatures\", \"AnnotationDbi\"))\n&gt; BiocManager::install(\"DESeq2\")\n</code></pre> <p>Perl Module Any of the Perl versions that you see when running the module command on Katana will include BioPerl. If you do not use the module command to access Perl then you won't have be able to use BioPerl and other Perl tools.  <code>module avail perl</code></p> <p>Conda If you would like to install software using Conda there are instructions on how to do it on the Python page.</p> <p>Note</p> <p>Some software packages like Bioconductor do not work well if installed in Conda due to software dependencies and the time that it takes for a new version of the software to be included in Conda.</p>"},{"location":"software/others/#comsol","title":"Comsol","text":"<p>The most user friendly way to run Comsol interactively is to use Katana OnDemand.</p> <p>Note</p> <p>You will need to belong to a group that owns a COMSOL licence (mech, spree, quantum, biomodel) </p> <p>Comsol Batch Jobs</p> <p>Note: The version of COMSOL available via myAccess MUST NOT be used for any research including generating files to be used on Katana.</p> <p>An example comsol batch job file is available in our GitHub repository. (Note: You need to join the UNSW GitHub organisation to access this repository) as well as being presented below.</p> console<pre><code>    #!/bin/bash\n#&lt;RESOURCE REQUESTS&gt;\n\nmkdir -p ${TMPDIR}/comsol\n\n    export MY_COMSOL_DIR=/srv/scratch/$USER/comsoldir\n\n    module load comsol/5.6-spree\n\n    comsol -nn 1 -np $NCPUS \\\n-recoverydir ${MY_COMSOL_DIR}/recoveries \\\n-tmpdir ${TMPDIR}/comsol \\\nbatch \\\n-inputfile ${MY_COMSOL_DIR}/MyModel.mph \\\n-outputfile ${MY_COMSOL_DIR}/MyModelOut.mph \\\n-batchlog ${MY_COMSOL_DIR}/MyModel.log\n</code></pre>"},{"location":"software/others/#intel-compilers-and-software-libraries","title":"Intel Compilers and Software Libraries","text":"<p>Research Technology Services has a licence for Intel Compiler Collection which can be accessed by loading a module and contains 4 groups of software, namely compilers, libraries, a debugger and MPI. This software has been optimised by Intel to take advantage of the specific capabilities of the different intel CPUs installed in the Intel based clusters.</p> <ul> <li>Compilers (module name: intel-compilers)<ul> <li>Intel C Compiler (icc)</li> <li>Intel C++ Compiler (icpc)</li> <li>Intel Fortran Compiler (ifort)</li> </ul> </li> <li>Libraries<ul> <li>Intel Math Kernel Library (MKL) (module name: intel-mkl)</li> <li>Intel Threading Building Blocks (TBB) (module name: intel-tbb)</li> <li>Intel Integrated Performance Primitives (IPP)</li> </ul> </li> <li>Debugger<ul> <li>Intel Debugger (idbc)</li> </ul> </li> </ul>"},{"location":"software/others/#java","title":"Java","text":"<p>Java is installed as part of the Operating System but that version of Java can change without warning leading to reproducable concerns. Because of this risk we recommend using one of the versions of Java available via the module command.</p> <p><code>module avail java</code></p> <p>Each Java module sets </p> <pre><code>    _JAVA_TOOL_OPTIONS -Xmx1g\n</code></pre> <p>This sets the heap memory to 1GB. If you need more, set the environment variable <code>_JAVA_OPTIONS</code> which overrides <code>_JAVA_TOOL_OPTIONS</code></p> <pre><code>    export _JAVA_OPTIONS=\"-Xmx5g\"\n</code></pre>"},{"location":"software/others/#matlab","title":"Matlab","text":"<p>Running interactively PBS_O_WORKDIR You can run an interactive session of Matlab using Katana OnDemand for a graphical session or using the qsub command for a text based session using the commands below.</p> <p>Batch Jobs</p> <p>You can run Matab within a batch job. The example below shows the flags used to start Matlab without a graphical interface. Your job will start in your Katana home directory so we are assuming that your Matlab script is in your home directory.</p> <pre><code>module load matlab/R2022a\n\nmatlab -nodisplay -nosplash -r scriptfile\n</code></pre> <p>If your Matlab script is not located in your home directory you can either provide the full path to the Matlab command or change to the directory within your batch file. The advantage of changing directory is that it makes it easy to save any results into the same directory.</p> <p>To provide a full path you can use the following example: </p><pre><code>module load matlab/R2022a\n\nmatlab -nodisplay -nosplash -r /path/to/script/scriptfile\n</code></pre><p></p> <p>If you want to change to a different directory you can use the following example. </p><pre><code>cd /path/to/script/\nmodule load matlab/R2022a\n\nmatlab -nodisplay -nosplash -r scriptfile\n</code></pre><p></p> <p>If your files are in the same directory that you submitted the batch job from then you can use the variable <code>$PBS_O_WORKDIR</code>, which contains the location that the job was submitted, in your script. </p><pre><code>cd $PBS_O_WORKDIR\nmodule load matlab/R2022a\n\nmatlab -nodisplay -nosplash -r scriptfile\n</code></pre><p></p> <p>Later versions of matlab provide the '-batch' flag as an alternative. </p> <pre><code>module load matlab/R2022a\n\nmatlab -batch -r scriptfile\n</code></pre>"},{"location":"software/others/#operating-systems","title":"Operating Systems","text":"<p>Katana nodes currently run Rocky Linux. To find out exactly which version you can use the command <code>cat /etc/redhat-release</code>.</p> <p>Research software is installed in environment modules. This enables multiple versions of the same software to be installed,  and each user can choose which version they wish to use.</p>"},{"location":"software/others/#perl","title":"Perl","text":"<p>The default version of Perl on Katana is 5.26.3, which is provided by Rocky Linux and can be found at <code>/usr/bin/perl</code>.</p> <p>We have also installed Perl via an environment module. </p> <p>It is common for Perl scripts to begin with:</p> <pre><code>#!/usr/bin/perl\n</code></pre> <p>However, that will restrict you to the default version of Perl supplied with the Linux distribution.  If, instead of using that line, you use the following: </p> <pre><code>#!/usr/bin/env perl\n</code></pre> <p>This will then look in your path so if you load Perl via an environment module it will not be necessary to modify your scripts.</p>"},{"location":"software/others/#stata","title":"Stata","text":"<p>Stata is available as a module. </p> <p>When using Stata in a pbs batch script, the syntax is</p> <pre><code>stata -b do StataClusterWorkshop.do\n</code></pre> <p>If you wish to load or install additional Stata modules or commands you should use findit command within Stata on your local computer to find the command that you are looking for. You should then create a directory called <code>myadofiles</code> in your home directory and copy the .ado (and possibly the .hlp) file into that directory. Now that the command is there  it just remains to tell Stata to look in that directory which can be done by using the following Stata command.</p> <pre><code>sysdir set PERSONAL $HOME/myadofiles\n</code></pre>"},{"location":"software/others/#tmux","title":"tmux","text":"<p>When you login to Katana using the terminal, it is a \"live\" session - if you close the terminal or turn off your computer the session will close. If you have a long running program such as downloading a large data set or have an interactive session running from the command line and you need to go somewhere else then you may end up losing all of your work and need to start again. In order to stop this from happening you can use <code>tmux</code> to create an interruptible session when you first connect to Katana.</p> <p>Note</p> <p>You will need to take note of the Katana login node that you have logged in to (katana1, katana2 or katana3) as you will need to connect to the same login node to return to your <code>tmux</code> session.</p> <p>To start tmux, type <code>tmux</code> at the terminal which will create a new session will start with a green information band at the bottom of the screen. Anything you start in this session  will keep running even if you disconnect or are disconnected from that session for any reason unless the server needs to be restarted.</p> <p>When you wish to reconnect you login to the server that you used last time and reconnect by using the command <code>tmux a</code>.</p> <p><code>tmux</code> has a number of other useful features such as multiple sessions and split screens. More information on features and how to use <code>tmux</code> is available on the tmux website.</p>"},{"location":"software/others/#zip","title":"Zip","text":"<p>Compressing Large Directories</p> <p>If you want to compress large directories or directories with a large number of files, we recommend using tgzme which was developed by one of our researchers, Dr. Edwards.</p> <p>The software is based on the command:</p> <pre><code>tar -c $DIRECTORY | pigz &gt; $DIRECTORY.tar.gz\n</code></pre> <p>We thank <code>Dr. Edwards</code> for his contribution.</p>"},{"location":"software/perl/","title":"Perl","text":"<p>The default version of Perl on Katana is 5.26.3, which is provided by Rocky Linux 8.9 and can be found at <code>/usr/bin/perl</code>.</p> <p>Perl 5.36.0 is also installed as an environment module. </p> <p>It is common for Perl scripts to begin with:</p> <pre><code>#!/usr/bin/perl\n</code></pre> <p>However, that will restrict you to the default version of Perl supplied with the Linux distribution.  For greater flexibility, try using the following: </p> <p></p><pre><code>#!/usr/bin/env perl\n</code></pre> Then if you choose to use a different version of Perl (by loading an environment module) it will not be necessary to modify your scripts.<p></p>"},{"location":"software/python/","title":"Python","text":"<p>It is common for python scripts to begin with </p> <pre><code>#!/usr/bin/python\n</code></pre> <p>If you are using a Python module, you will need to change the first line to </p> <pre><code>#!/usr/bin/env python\n</code></pre> <p>or, idealy,</p> <pre><code>#!/usr/bin/env python3\n</code></pre>"},{"location":"software/python/#conda-and-anaconda","title":"Conda and Anaconda","text":"<p>We get a lot of questions about installing Conda and Anaconda. Unfortunately neither are designed to be installed in multi-user environments but you can install them into your  your home or scratch directories. The advantage of installing Conda in your scratch directory is that each Conda environment can become quite large which is amplified if you require multiple Conda environments. To install Conda for yourself you can using the following commands.</p> <p>Start by downloading yout preferred version of Conda, with the two main versions being Anaconda and Miniconda.</p> <p>If you have downloaded Miniconda the instructions are provided below. The instructions for Anaconda are the same once the Conda environment has been created. </p><pre><code>bash Miniconda3-latest-Linux-x86_64.sh\n[z1234567@katana2 z1234567]$ conda activate\n(base) [z1234567@katana2 z1234567]$ conda create --name my_conda\n(base) [z9470105@katana1 ~]$ conda activate my_conda\n(my_conda) [z9470105@katana1 ~]$ conda install python\n</code></pre><p></p> <p>Note</p> <p>If you are using Conda for GPU-enabled software, make sure it is installed on a GPU node during an interactive session.</p>"},{"location":"software/python/#python-virtual-environments","title":"Python Virtual Environments","text":"<p>Many packages will give you the option option to use <code>pip install</code> - if this is an option, we recommend you use <code>python virtual environments</code> especially if you are developing your own software or want to use packages that aren't installed.</p>"},{"location":"software/python/#background","title":"Background","text":"<p>To use packages, a collection of files written in Python, not already installed you will need to use what is known as a virtual environment or venv. This gives us a version of Python in our home directory where we can install any packages we like. </p>"},{"location":"software/python/#setting-up-the-default-environment","title":"Setting up the default environment","text":"<p>Information on virtual environments can be found in the Python documentation. To begin with you should run the command  <code>python3 -m venv /path/to/new/virtual/environment</code>. This directory is where the Python environment is installed and you should not use it as a location to write Python scripts. </p> <p>You should consider making a directory to contain your virtual environments using the following command:</p> <pre><code>[z1234567@katana2 ~]$ mkdir /home/z1234567/environments\n</code></pre>"},{"location":"software/python/#setting-up-the-virtual-environment-creation-and-activation","title":"Setting up the virtual environment - creation and activation","text":"<p>As Katana has multiple Python modules you should pick the one that works best for you. If you don't have any specific requirements then you should choose the most recent one.</p> <pre><code>[z1234567@katana2 ~]$ module load python/3.10.8\n[z1234567@katana2 ~]$ which python3\n/apps/python/3.10.8/bin/python3\nz1234567@katana2 ~]$ python3 -m venv /home/z1234567/environments/my_env\n</code></pre> <p>Note</p> <p>The command <code>which</code> show the path to a command. You don't need to use it unless you want to confirm where the command it installed.</p> <p>The next stage is to activate the virtual environment which means that we are using the version of Python that we just created using the command <code>source /home/z1234567/environments/my_env</code>.  After you have activated the virtual environment the prompt changes to make it clear that you are working in your virtual environment. If you use the <code>which</code> command with <code>python3</code> and <code>pip3</code></p> <pre><code>[z1234567@katana2 ~]$ source /home/z1234567/environments/my_env/bin/activate\n</code></pre> <p>After activation, python will run from the new location.  the defaults, but the versions in our venv</p> <pre><code>(my_env) [z1234567@katana2 ~]$ which python3\n/home/z1234567/environments/my_env/bin/python3\n</code></pre>"},{"location":"software/python/#pip3-the-python-package-manager-the-package-installer-for-python","title":"pip3 - the Python package manager (\"the Package Installer for Python\")","text":"<p>Using pip3 we can see whats installed and install new packages. </p> <pre><code>(my_env) [z1234567@katana2 ~]$ pip3 list\nPackage    Version\n---------- -------\npip        19.0.3 \nsetuptools 40.8.0 \nYou are using pip version 19.0.3, however version 20.0.2 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\n</code></pre> <p>If it says that a newer verions of pip is available then you should upgrade.</p> <pre><code>(my_env) [z1234567@katana2 ~]$ pip3 install --upgrade pip\nCollecting pip\n    Downloading https://files.pythonhosted.org/packages/54/0c/d01aa759fdc501a58f431eb594a17495f15b88da142ce14b5845662c13f3/pip-20.0.2-py2.py3-none-any.whl (1.4MB)\n100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.4MB 1.5MB/s \nInstalling collected packages: pip\n    Found existing installation: pip 19.0.3\n    Uninstalling pip\\-19.0.3:\n    Successfully uninstalled pip\\-19.0.3\nSuccessfully installed pip-20.0.2\n(venv-tutorial-1) [z1234567@katana2 ~]$ pip install --upgrade setuptools\nCollecting setuptools\n    Downloading setuptools-46.1.1-py3-none-any.whl (582 kB)\n|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 582 kB 13.5 MB/s \nInstalling collected packages: setuptools\n    Attempting uninstall: setuptools\n    Found existing installation: setuptools 40.8.0\n        Uninstalling setuptools\\-40.8.0:\n            Successfully uninstalled setuptools\\-40.8.0\nSuccessfully installed setuptools-46.1.1\n(venv-tutorial-1) [z1234567@katana2 w~]$ pip3 list\nPackage    Version\n---------- -------\npip        20.0.2 \nsetuptools 46.1.1 </code></pre>"},{"location":"software/python/#installing-software","title":"Installing software","text":"<p>To install a package that you want to use you can run the command <code>pip3 install ...</code>:</p> <pre><code>(my_env) [z1234567@katana2 ~]$ pip3 install numpy\nCollecting numpy\n    Downloading numpy-1.18.2-cp37-cp37m-manylinux1*x86*64.whl (20.2 MB)\n|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20.2 MB 38 kB/s \nInstalling collected packages: numpy\nSuccessfully installed numpy-1.18.2\n(my_env) [z1234567@katana2 ~]$ pip3 list\nPackage    Version\n---------- -------\nnumpy      1.18.2 \npip        20.0.2 \nsetuptools 46.1.1 </code></pre>"},{"location":"software/python/#exiting-the-venv-and-coming-around-again","title":"Exiting the venv, and coming around again","text":"<p>To leave a venv, you use the <code>deactivate</code> command like this:</p> <pre><code>(my_env) [z1234567@katana2 ~]$ deactivate [z1234567@katana2 ~]$\n</code></pre> <p>You can create additional virtual environments in the same way and we can see both environments. </p><pre><code>[z1234567@katana2 ~]$ python3 -m venv /home/z1234567/environments/my_new_env\n[z1234567@katana2 ~]$ ls -l /home/z1234567/environments\ntotal 0\ndrwx------. 5 z1234567 unsw 69 Mar 23 15:07 my_env\ndrwx------. 5 z1234567 unsw 69 Mar 23 11:45 my_new_env\n\n[z1234567@katana2 ~]$ source /home/z1234567/environments/my_new_env/bin/activate\n(my_new_env) [z1234567@katana2 ~]$ pip3 list\nPackage    Version\n---------- -------\npip        19.0.3 \nsetuptools 40.8.0 \nYou are using pip version 19.0.3, however version 20.0.2 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\n</code></pre><p></p> <p>When we install SciPy it will automatically install NumPy as it is a ependency.</p> <pre><code>(scipy-example) [z1234567@katana2 ~]$ pip install scipy\nCollecting scipy\n    Downloading scipy-1.4.1-cp37-cp37m-manylinux1*x86*64.whl (26.1 MB)\n|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 26.1 MB 95 kB/s \nCollecting numpy&gt;=1.13.3\n    Using cached numpy-1.18.2-cp37-cp37m-manylinux1*x86*64.whl (20.2 MB)\nInstalling collected packages: numpy, scipy\nSuccessfully installed numpy-1.18.2 scipy-1.4.1\n(scipy-example) [z1234567@katana2 ~]$ pip list\nPackage    Version\n---------- -------\nnumpy      1.18.2 \npip        20.0.2 \nscipy      1.4.1  \nsetuptools 46.1.1 </code></pre> <p>You can install an older version by specifying the version that you want to install. </p><pre><code>(old-scipy-example) [z1234567@katana2 ~]$ pip install scipy==1.2.3\nCollecting scipy==1.2.3\n    Downloading https://files.pythonhosted.org/packages/96/e7/e06976ab209ef44f0b3dc638b686338f68b8a2158a1b2c9036ac8677158a/scipy-1.2.3-cp37-cp37m-manylinux1_x86_64.whl (24.8MB)\n100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 24.8MB 239kB/s \nCollecting numpy&gt;=1.8.2 (from scipy==1.2.3)\nUsing cached https://files.pythonhosted.org/packages/b7/ce/d0b92f0283faa4da76ea82587ff9da70104e81f59ba14f76c87e4196254e/numpy-1.18.2-cp37-cp37m-manylinux1_x86_64.whl\nInstalling collected packages: numpy, scipy\nSuccessfully installed numpy-1.18.2 scipy-1.2.3\n(old-scipy-example) [z1234567@katana2 src]$ pip list\nPackage    Version\n---------- -------\nnumpy      1.18.2 \npip        20.0.2 \nscipy      1.2.3  \nsetuptools 46.1.1 </code></pre><p></p>"},{"location":"software/python/#special-cases","title":"Special Cases","text":"<p>If you want your virtual environment to take advantage of a module that has been installed on Katana.</p> <p>In that case, your workflow would be:</p> <ul> <li>load the module</li> <li>create the Virtual Environment with the flag <code>--system-site-packages</code></li> <li>install software in question</li> </ul> <p>For example, using samtools and wanting to run your virtual environment in Jupyter:</p> <pre><code>[z1234567@katana2 ~]$ module add samtools/1.15.1\n[z1234567@katana2 ~]$ python3 -m venv /home/z1234567/environments/my_samtools_env --system-site-packages\n[z1234567@katana2 ~]$ source /home/z1234567/environments/my_samtools_env/bin/activate\n(my_samtools_env) [z1234567@katana2 ~]$ pip3 install pysam\n</code></pre> <p>You can now create a Jupyter kernel which will ellow you to use the virtual environment that you have created.</p>"},{"location":"software/r/","title":"R and RStudio","text":"<p>R is installed as a module with a number of R packages installed. It can be useful to always use the same version of R to avoid the possibility of changes that affect your calculations between different versions.</p>"},{"location":"software/r/#installing-libraries","title":"Installing libraries","text":"<p>Because R allows users to have their own library of installed packages you will be able to install most packages for yourself. You can see the packages that you have installed along with those that come with the version of R that you have loaded by using the following command.</p> <pre><code>    &gt;library()\n</code></pre> <p>Most of the time you will be able to install R packages from CRAN using the following command which will automatically look after all of the package dependencies for you.</p> <pre><code>    &gt; install.packages('mypackage')\n...\n* DONE (mypackage)\n</code></pre> <p>If one or more of the packages that you have installed for yourself rely on accessing software via the module command then you will need to load the module before starting R. If you forgot or are using RStudio where it is not possible to load the module ahead of time you can load the module from within R by making use of the file <code>/usr/share/Modules/init/r.R</code> in the following way to both load and unload modules.</p> <pre><code>    &gt; load(\"/usr/share/Modules/init/r.R\")\n&gt; module(\"add\",\"gdal/3.5.3\")\n&gt; module(\"del\",\"gdal/3.5.3\")\n</code></pre> <p>If you have problems installing a package and would like help installing it please email restech.support@unsw.edu.au.</p>"},{"location":"software/r/#rstudio-and-jupyterlab-in-katana-ondemand","title":"RStudio and JupyterLab in Katana OnDemand","text":"<p>If you would like to use RStudio on Katana, we recommend you use Katana OnDemand which allows you to specify the version of R that you wish to use including any packages that you have previously installed. Depending on the packages that you have installed you may need to use the module command decribed above.</p> <p>It is also possible to use the R Kernel within JuypterLab including having access to the packages that you have installed but we recommend using RStudio instead as it has been written specificlly for R. If you wish to use the R Kernel within JuypterLab then you can use the following commands within R before you start JupyterLab. </p> <pre><code>    &gt; install.packages(\"devtools\")\n&gt; devtools::install_github(\"IRkernel/IRkernel\")\n&gt; IRkernel::installspec()\n</code></pre>"},{"location":"software/r/#r-and-rstudio-in-conda","title":"R and RStudio in Conda","text":"<p>Whilst it is possible to install R and RStudio within Conda you will probably not be able to install the version of R and supporting software that you wish to use. If the version that you wish to use is not currently used on Katana then you should email the Restech team at restech.support@unsw.edu.au and they will install it for you. The instructions on installing R within Conda, if required, are given below.</p> Instructions for using R within Conda  !!! warning     Using Conda to install complex software with large number of dependencies such as R or Rstudio may not provide provide you a recent version.  If you wish to install R as part of a Conda environment then you can use the following commands to install R in a new repository called R_software after . This may be the easiest approach to take if you need to use a version of R that is not installed on Katana or a specialised collection of packages like [BioConductor](https://www.bioconductor.org/). If you  are having trouble figuring out which approach to take please email the Restech team at [restech.support@unsw.edu.au](mailto:restech.support@unsw.edu.au).  <pre><code>    conda create --name R_software\n    conda activate R_software\n    conda install R\n</code></pre>  If you then want to use RStudio you can install it using the following command:   <pre><code>    conda install RStudio\n</code></pre>  If there is a conflict between the installed Conda packages then you can create a new Conda environment using the following commands:  <pre><code>    conda create --name RStudio\n    conda activate RStudio\n    conda install RStudio\n</code></pre>  Once RStudio has been installed you can start a **FastX Desktop** with your required resources, open a terminal window by clicking on the icon at the top of the screen. Once the terminal opens you can then activate your Conda environment and then use the following command to start RStudio:  <pre><code>    rstudio\n</code></pre>"},{"location":"storage/data_archive/","title":"How to use the UNSW Data Archive","text":"<p>The UNSW Data Archive is the primary research storage facility provided by UNSW. The Data Archive gives UNSW researchers a free, safe and secure storage service to store and access research data well beyond the life of the project that collected that data.</p> <p>To help researchers make use of this system the Katana Data Mover has a script that you can use to copy files from Katana into a project on the Data Archive system.</p> <p>Note</p> <p>To use this script you must have access to the UNSW Data Archive which requires setting up a Research Data Management Plan.</p> <p>The best documentation on how to use the <code>Data Archive</code> is found on their website:</p> <ul> <li>using the web application</li> <li>using SFTP</li> <li>using the Command Line</li> </ul> <p>To see what versions of the Data Archive script are available log on to <code>kdm.restech.unsw.edu.au</code> and type</p> <pre><code>module avail unswdataarchive\n</code></pre> <p>Use the help command for usage</p> <pre><code>module help unswdataarchive/2021-02-17\n</code></pre>"},{"location":"storage/data_archive/#initial-setup","title":"Initial Setup","text":"<p>To use the Data Archive you need to set up a configuration file. Here's how to create the generic config in the directory you are in:</p> <pre><code>[z1234567@kdm ~]$ module add unswdataarchive/2021-02-17\n[z1234567@kdm ~]$ get-config-file\n</code></pre> <p>To generate a token send an email to the IT Service Centre asking for a Data Archive token to be generated. A service desk request for an authentication token to be generated needs to indicate a Data Archive namespace (/UNSW_RDS/Dxxx or /UNSW_RDS/Hxxx) as a scope for the token. Your Data Archive namespace is recorded in the Data Archive welcome email.</p> <p>Then edit the configuration file <code>config.cfg</code> and change the line that looks like <code>token=</code></p> <p>If you haven't generated a token you can also upload content using your zID and zPass by adding the following line to the file <code>config.cfg</code> and you will be asked for your zPass when you start the upload.</p> <pre><code>user=z1234567\n</code></pre>"},{"location":"storage/data_archive/#starting-a-data-transfer","title":"Starting a data transfer","text":"<p>To get data into the archive, we use <code>upload.sh</code></p> <pre><code>upload.sh /path/to/your/local/directory /UNSW_RDS/D0000000/your/collection/name\n</code></pre> <p>To get data from the archive, we use <code>download.sh</code></p> <pre><code>download.sh /UNSW_RDS/D0000000/your/collection/name /path/to/your/local/directory\n</code></pre>"},{"location":"storage/kdm/","title":"Katana Data Mover","text":"<p>Also known as <code>kdm</code> or <code>kdm.restech.unsw.edu.au</code></p> <p>If you have data that you would like to copy to or within the Katana cluster, archive or even compress and decompress you should use the Katana Data Mover - also known as the KDM server - rather than using the head node. This section contains instructions on how to use KDM server.</p> <p>If you are familiar with using Linux commands to copy or move files then you can do that directly by logging on to <code>kdm.restech.unsw.edu.au</code> via <code>ssh</code> in the same way that you would log in to Katana and then use the <code>cp</code>, <code>mv</code> and <code>rsync</code> commands that you would normally use under Linux.</p> <p>If you are not familiar with using the Linux command line for moving or copying files then the easiest way to move files around is to use client software such as FileZilla_. Once you have connected to <code>kdm.restech.unsw.edu.au</code> using your zID and zPass you should see a remote view which corresponds to the files sitting on Katana. You can then use the FileZilla interface to move files and folders around.</p> <p>Note</p> <p>We require people to \"move data\" through the data mover. We have hundreds of users, most of whom have data ranging from very large to impossibly large. This is why we have the <code>kdm</code>. If you are transferring a couple of small text files - job scripts for instance - you can copy directly to the Katana. But we would ask you to keep it to a minimum, and nothing bigger than 2-3 MB.</p>"},{"location":"storage/kdm/#copying-files-to-and-from-a-cluster","title":"Copying Files To and From a Cluster","text":"<p>The method of transferring files to and from clusters depends on your local machine. If you are a Linux user then you should use rsync and if you are a Windows user then you should download and install WinSCP or FileZilla</p> <p>Warning</p> <p>Filezilla is often marked as being a security risk. There is nothing that Research Technology Services can do about that - it's an issue for UNSW IT, Symantec, and FileZilla. If this happens to you, please try WinSCP - it's very good.</p>"},{"location":"storage/kdm/#filezilla","title":"Filezilla","text":"<p>Once you have installed Filezilla you can go into the site manager and create a new site in the site manager using the settings below.</p> <p> </p> filezilla site manager <p>You can also use the Quick Connect bar as shown here: </p> <p> </p> filezilla quick connect"},{"location":"storage/kdm/#from-my-computer-to-katana-home","title":"From my computer to Katana Home","text":"<p>To copy the directory <code>/home/1234567/my-directory</code> from your local computer to Katana scratch. The trailing <code>:</code> is important!</p> <pre><code>me@localhost:~$ rsync -avh /path/to/my-directory z1234567@kdm.restech.unsw.edu.au:\n</code></pre>"},{"location":"storage/kdm/#from-my-computer-to-katana-scratch","title":"From my computer to Katana Scratch","text":"<pre><code>me@localhost:~$ rsync -avh /path/to/my-directory z1234567@kdm.restech.unsw.edu.au:/srv/scratch/z1234567\n</code></pre>"},{"location":"storage/kdm/#from-katana-to-my-computer","title":"From Katana to my computer","text":"<p>First, you need to make sure the data is in either your Home directory or your scratch </p> <p>If the data is in <code>/home/z1234567/my-remote-results</code> and you want it in your home directory:</p> <pre><code>me@localhost:~$ rsync -avh z1234567@kdm.restech.unsw.edu.au:my-remote-results /home/me/\n</code></pre> <p>If the data is in <code>/srv/scratch/my-remote-results</code> and you want it in your home directory:</p> <pre><code>me@localhost:~$ rsync -avh z1234567@kdm.restech.unsw.edu.au:/srv/scratch/my-remote-results /home/me\n</code></pre> <p>Note</p> <p>TMUX is available if your data is large and the rsync might take a long time.</p>"},{"location":"storage/kdm/#mounting-university-provided-space-on-kdm","title":"Mounting University Provided Space on KDM","text":"<p>The university provides a large amount of space if you need. You can find more information about <code>staff storage</code> on the <code>UNSW website</code>.</p> <p>Filezilla</p> <p>WinSCP</p> <p>staff storage</p> <p>UNSW website</p>"},{"location":"storage/onedrive/","title":"OneDrive","text":"<p>You can mount Microsoft's OneDrive on Katana.</p>"},{"location":"storage/onedrive/#onedrive-background","title":"OneDrive Background","text":"<p>We don't recommend researchers mount their OneDrive because the configuration process is not ideal. However, it is possible with the following method.</p> <p><code>There are limitations to using RClone with OneDrive</code> that users will need to be  aware of and which the UNSW team cannot help with.</p> <p>Note: Mounting OneDrive locally will only work on the machine on which the <code>mount</code> command is run.</p> <p>Note</p> <p>Please mount OneDrive on Katana Data Mover, not the login nodes. </p> <p>The configuration section will probably only need to run once, whereas the \"how to mount\" section will need to be run each time.</p>"},{"location":"storage/onedrive/#prerequisites","title":"Prerequisites","text":"<ol> <li>OneDrive needs your consent to give rclone access to your files. </li> <li>We can't guarantee or test if this works for researchers in Germany or  China. OneDrive has a different set up for Germany and China due to local laws  about data storage.</li> <li>You will need at least one empty directory for mounting. Restech recommends creating the directory <code>/home/z1234567/OneDrive</code> to mount OneDrive onto.</li> </ol>"},{"location":"storage/onedrive/#configure-rclone-for-onedrive","title":"Configure RClone for OneDrive","text":"<ol> <li>Login to KDM and run:</li> </ol> <pre><code>[z1234567@kdm ~]$ rclone config\n</code></pre> <p>You will be asked a set of questions. The short answers are:</p> <ol> <li>new remote ('n')</li> <li>name ('OneDrive')</li> <li>storage type Microsoft OneDrive ('26' in rclone version 1.55.1)</li> <li>OAuth Client Id (Press Enter for the default)  </li> <li>OAuth Client Secret (Press Enter for the default)</li> <li>Choose national cloud region for OneDrive (\"1. global\")</li> <li>Edit advanced config? (\"n\")</li> <li>Remote config? (\"n\")</li> <li>On a machine with rclone and a web browser (not kdm): Run the <code>rclone authorize \"onedrive\" ...</code> command and copy to the clipboard. </li> <li>Back to kdm: Paste result: </li> <li>Choose a number from below, or type in an existing value (\"1. OneDrive Personal or Business\")</li> <li>Chose drive to use: (\"0\")</li> <li>Is that okay? (\"Y\")</li> </ol> <p>You should then see something like this to which you should answer yes:</p> <pre><code>   --------------------\n    [MS OneDrive]\ntype = onedrive\n    client_id = c8800f43-7805-46c2-b8b2-1c55f3859a4c\n    client_secret = SECRET\n    region = global\n    token = {\"access_token\":\"eyJ0e...asdasd\"}\ndrive_type = business\n    --------------------\n    y) Yes this is OK (default)\ne) Edit this remote\n    d) Delete this remote\n    y/e/d&gt; </code></pre>"},{"location":"storage/onedrive/#how-to-mount-onedrive","title":"How to mount OneDrive","text":"<p>Once logged in:</p> <ol> <li>Mount the drive. The basic syntax is:</li> </ol> <pre><code>rclone mount &lt;remote-name&gt;: /path/to/local/mount\n</code></pre> <p>We need to add a couple of flags to make this warning free and usable. Most  notably <code>--daemon</code> and <code>--vfs-cache-mode writes</code>.</p> <p>If you have followed the ResTech recommendations, your command will look like:</p> <pre><code>[z1234567@kdm ~]$ rclone mount OneDrive: /home/z1234567/OneDrive --daemon --vfs-cache-mode writes\n</code></pre> <p>Info</p> <p>Your OneDrive file contents should now be available at /home/z1234567/OneDrive (or chosen mount point). </p> <p>OneDrive</p> <p>OneDrive needs your consent</p> <p>There are limitations to using RClone with OneDrive</p>"},{"location":"storage/storage_locations/","title":"Storage Locations","text":"<p>The storage on Katana is split into several different types, each of which serves a different purpose and may only be accessible from certain locations. </p> <p>Important</p> <p>We have just said each of which serves a different purpose. Despite that, there will be overlap. And personal preference. In most cases it will be obvious where to put your information. If it isn't and you need help with your decision making, you can email the Research Data team team at rdm@unsw.edu.au for advice. They are friendly people.</p> <p>If you specifically wish to increase Katana storage allocations, please email restech.support@unsw.edu.au - note that such increases are not automatic.</p>"},{"location":"storage/storage_locations/#storage-summary","title":"Storage  Summary","text":"Storage type Location Alias Purpose Backed up? Size limit Who has access Where can it be used? Home drive /home/z1234567 $HOME Source code and programs Y 10Gb Only the user Anywhere User scratch /srv/scratch/z1234567 /srv/scratch/$USER Data files N 128 GB Only the user Anywhere Shared scratch /srv/scratch/group_name /srv/scratch/group_name Data files and programs shared by a team N Upon group requirements All users in the group Anywhere Local scratch Intialised upon job running $TMPDIR Faster job completion with large datasets and temp files N 200Gb shared between node users Temporary for each job Only on compute nodes UNSW Research Storage /home/z1234567/sharename $HOME/sharename Storage of shared user and data files Y Depends on storage location Only available on KDM (and login nodes for small files) Instrument Dara Store (IDS) /home/z1234567/ids $HOME/ids Storage of instrument data generated within MWAC Y Depends on storage location Only available on KDM"},{"location":"using_katana/about_katana/","title":"About Katana","text":"<p>Katana is a shared computational cluster located on campus at UNSW that has been designed to provide easy access to computational resources for groups working with non-sensitive data. It contains over 7,000 CPU cores, 25 GPU compute nodes (H200, GH200, A100, V100 and L40S), and 8PB of disk storage. Katana provides a flexible compute environment where users can run jobs that wouldn't be possible or practical on their desktop or laptop. For full details of the compute nodes including a full list see the compute node information section below.</p> <p>Katana is powerful on its own, but can be seen as a training or development base before migrating up to systems like Australia's peak HPC system Gadi at NCI. Research Technology Services also provide training and support for new users or those uncertain if High Performance Computing is the right fit for their research needs.</p>"},{"location":"using_katana/about_katana/#system-configuration","title":"System Configuration","text":"<ul> <li>RPM based Linux OSes. Rocky 8 on the management plane and nodes</li> <li>OpenPBS version 23.06.06</li> <li>Large global scratch at <code>/srv/scratch</code>, local scratch at <code>$TMPDIR</code></li> <li>12, 48, 100, 200 hour Walltime queues with prioritisation</li> </ul>"},{"location":"using_katana/about_katana/#compute","title":"Compute","text":"<ul> <li>Heterogenous hardware: Dell, Lenovo, Supermicro, Xenon.</li> <li>More than 150 nodes</li> </ul>"},{"location":"using_katana/about_katana/#gpu-compute","title":"GPU Compute","text":"<ul> <li>25 GPU nodes<ul> <li>24 x Nvidia H200 141GB (6 nodes)</li> <li>1 x Nvidia GH200 480GB + 96GB (1 node)</li> <li>32 x Nvidia L40S 48GB (7 nodes)</li> <li>20 x Nvidia A100 40GB (3 nodes)</li> <li>32 x Nvidia V100 32GB (8 nodes)</li> </ul> </li> <li>12 GPU nodes have priority access for the school/group that purchased them</li> <li>13 GPU nodes are for use by all researchers</li> </ul> <p>Info</p> <p>Unfortunately, GPU nodes are in incredibly high demand.  We cannot provide special accommodation for any project.  You will need to wait in the common queue - or buy a GPU node for your group on which you will get priority</p> <p>To access a GPU node interactively, you can use a command like</p> <pre><code>[z1234567@katana ~]$ qsub -I -l select=1:ncpus=8:ngpus=1:mem=46gb,walltime=2:00:00\n</code></pre> <p>Info</p> <ul> <li>You cannot use Tensorflow or Pytorch on the login nodes because they don't have GPUs. You will need to get access to a GPU node to do this. </li> <li>GPU-enabled software should be installed on a GPU node within an interactive session, in case the software is probing for GPU hardware or libraries.</li> <li>Note the 2 hour limit - that is the fastest way to get onto the GPU nodes. Please check out qsub command to find more options</li> </ul>"},{"location":"using_katana/accessing_katana/","title":"Accessing Katana","text":""},{"location":"using_katana/accessing_katana/#requesting-an-account","title":"Requesting an Account","text":"<p>To apply for an account, please fill out the ServicePoint form Katana Access Request, including your role within UNSW and the name of your supervisor or head of your research group.</p> <p>Anyone at UNSW can apply for a general account on Katana if they think that Katana would suit their research needs and will typically use  less than 10,000 CPU core hours a quarter. If a researcher requires or uses more than that they should also give consideration to using NCI.</p> <p>All Katana users gets access to help with software installation, getting started on Katana or running their jobs. </p> <p>The only difference between Katana users is the number of compute jobs that can be run at any time and how long they can run for - general  users can only use a 12 hour Walltime.</p> <p>If your needs require more CPU hours or consulation, some Faculties, Schools and Research Groups have invested in Katana and have a higher level of access. Users in this situation should speak to their supervisor.</p>"},{"location":"using_katana/accessing_katana/#accessing-katana","title":"Accessing Katana","text":"Via Web/Graphical Session Via Terminal"},{"location":"using_katana/accessing_katana/#connecting-to-katana-via-terminal","title":"Connecting to Katana via Terminal","text":"<p>Operating System</p> Linux and MacWindowsWindows subsystem for Linux (WSL) <p>From a Linux or Mac OS machine you can connect via ssh in a terminal:</p> <p>Launch Terminal on your Mac OS or Linux computer, and run:</p> <pre><code>laptop:~$ ssh z1234567@katana.restech.unsw.edu.au\n</code></pre> <p>From a Windows machine an SSH client such as Putty or MobaXTerm is required. </p> <p>In Putty/MobaXTerm, use <code>zID@katana.restech.unsw.edu.au</code> as host name.</p> <p>Putty: </p> <p></p> <p>Moba: </p> <p></p> <p>Then click on Open, and accept to trust the identity of server.</p> <p>If you are comfortable using PowerShell, OpenSSH is available on recent Windows versions. If not present, it can be installed on Windows 10/11 or Windows Server. </p> <pre><code>C:\\Windows\\system32&gt; ssh z1234567@katana.restech.unsw.edu.au\n</code></pre> <p>Enter your zPass to connect.</p> <p>You can run a Linux environmet directly on Windows using Windows Subsystem for Linux (WSL).</p> <p>There are two ways to install WSL on your system:</p> <ol> <li>On UNSW Windows standard operating environment (SOE) machines you can open the Company Portal App and from there install one of the Linux distrubtions through the 'Apps', the same as you would other applications.</li> <li>Manually enable WSL in PowerShell and then install a Linux distribution through the Microsoft Store. </li> </ol> <p>Using WSL will not only let you connect to katana with SSH, but also provides many GNU/Linux tools that are useful when working with HPC and research data.</p>"},{"location":"using_katana/monitor_jobs/","title":"Monitor your Jobs(qstat, qdel, qalter)","text":""},{"location":"using_katana/monitor_jobs/#get-information-about-the-state-of-the-scheduler","title":"Get information about the state of the scheduler","text":"<p>When deciding which jobs to run, the scheduler takes the following details into account:</p> <ul> <li>are there available resources</li> <li>how recently has this user run jobs successfully</li> <li>how many resources has this user used recently</li> <li>how long is the job's Walltime</li> <li>how long has the job been in the queue</li> </ul> <p>You can get an overview of the compute nodes and a list of all the jobs running on each node using <code>pstat</code></p> <pre><code>[z1234567@katana2 src]$ pstat\nk001  normal-mrcbio           free          12/44   200/1007gb  314911*12\nk002  normal-mrcbio           free          40/44    56/ 377gb  314954*40\nk003  normal-mrcbio           free          40/44   375/ 377gb  314081*40\nk004  normal-mrcbio           free          40/44    62/ 377gb  314471*40\nk005  normal-ccrc             free           0/32     0/ 187gb\nk006  normal-physics          job-busy      32/32   180/ 187gb  282533*32\nk007  normal-physics          job-busy      32/32   180/ 187gb  284666*32\nk008  normal-physics          free           0/32     0/ 187gb\nk009  normal-physics          job-busy      32/32   124/ 187gb  314652*32\nk010  normal-physics          free           0/32     0/ 187gb      </code></pre> <p>To get information about a particular node, you can use <code>pbsnodes</code> but on its own it is a firehose. Using it with a particular node name is more effective:</p> <pre><code>[z1234567@katana2 src]$ pbsnodes k254\nk254\n    Mom = k254\n    ntype = PBS\n    state = job-busy\n    pcpus = 32\njobs = 313284.kman.restech.unsw.edu.au/0, 313284.kman.restech.unsw.edu.au/1, 313284.kman.restech.unsw.edu.au/2 resources_available.arch = linux\n    resources_available.cpuflags = avx,avx2,avx512bw,avx512cd,avx512dq,avx512f,avx512vl\n    resources_available.cputype = skylake-avx512\n    resources_available.host = k254\n    resources_available.mem = 196396032kb\n    resources_available.ncpus = 32\nresources_available.node_weight = 1\nresources_available.normal-all = Yes\n    resources_available.normal-qmchda = Yes\n    resources_available.normal-qmchda-maths_business-maths = Yes\n    resources_available.normal-qmchda-maths_business-maths-general = Yes\n    resources_available.vmem = 198426624kb\n    resources_available.vnode = k254\n    resources_available.vntype = compute\n    resources_assigned.accelerator_memory = 0kb\n    resources_assigned.hbmem = 0kb\n    resources_assigned.mem = 50331648kb\n    resources_assigned.naccelerators = 0\nresources_assigned.ncpus = 32\nresources_assigned.ngpus = 0\nresources_assigned.vmem = 0kb\n    resv_enable = True\n    sharing = default_shared\n    last_state_change_time = Thu Apr 30 08:06:23 2020\nlast_used_time = Thu Apr 30 07:08:25 2020\n</code></pre>"},{"location":"using_katana/monitor_jobs/#managing-jobs-on-katana","title":"Managing Jobs on Katana","text":"<p>Once you have jobs running, you will want visibility of the system so that you can manage them - delete jobs, change jobs, check that jobs are still running.</p> <p>There are a couple of easy to use commands that help with this process.</p> <p>Job Commands</p> qstatqdelqalter <p></p>Show all jobs on the system <code>qstat</code> gives very long output. Consider piping to <code>less</code><p></p> <pre><code>    [z1234567@katana2 ~]$ qstat | less\n    Job id            Name             User              Time Use S Queue\n    ----------------  ---------------- ----------------  -------- - -----\n    245821.kman       s-m20-i20-200h   z1234567                 0 Q medicine200\n    280163.kman       Magcomp25A2      z1234567          3876:18: R mech700\n    282533.kman       Proj_MF_Nu1      z1234567          3280:08: R cosmo200\n    284666.kman       Proj_BR_Nu1      z1234567          3279:27: R cosmo200\n    308559.kman       JASASec55        z1234567          191:21:3 R maths200\n    309615.kman       2020-04-06.BUSC  z1234567          185:00:5 R babs200\n    310623.kman       Miaocyclegan     z1234567          188:06:3 R simigpu200\n    ...\n</code></pre> <p></p>List just my jobs<p></p> <p>You can use either your ZID or the Environment Variable <code>$USER</code></p> <pre><code>    [z2134567@katana2 src]$ qstat -u $USER\nkman.restech.unsw.edu.au: Req'd  Req'd   Elap\n    Job ID          Username Queue    Jobname    SessID NDS TSK Memory Time  S Time\n    --------------- -------- -------- ---------- ------ --- --- ------ ----- - -----\n    315230.kman.res z2134567 general1 job.pbs       --    1   1    1gb 01:00 Q   -- </code></pre> <p>If you add the <code>-s</code> flag, you will get slightly more status information.</p> <pre><code>[z1234567@katana2 src]$ qstat -su z1234567\n\nkman.restech.unsw.edu.au: Req'd  Req'd   Elap\nJob ID          Username Queue    Jobname    SessID NDS TSK Memory Time  S Time\n--------------- -------- -------- ---------- ------ --- --- ------ ----- - -----\n315230.kman.res z1234567 general1 job.pbs     61915   1   1    1gb 01:00 R 00:03\nJob run at Fri May 01 at 14:28 on (k019:mem=1048576kb:ncpus=1:ngpus=0)\n315233.kman.res z1234567 general1 job.pbs       --    1   1    1gb 01:00 Q   --\n    -- </code></pre> <p></p>List information about a particular job <pre><code>[z1234567@katana2 src]$ qstat -f 315236                                                                                                                                       \nJob Id: 315236.kman.restech.unsw.edu.au                                                                                                                                       Job_Name = job.pbs                                                                                                                                                        Job_Owner = z1234567@katana2\n    job_state = Q\n    queue = general12\n    server = kman.gen\n    Checkpoint = u\n    ctime = Fri May  1 14:41:00 2020\nError_Path = katana2:/home/z1234567/src/job.pbs.e315236\n    group_list = GENERAL\n    Hold_Types = n\n    Join_Path = n\n    Keep_Files = n\n    Mail_Points = a\n    mtime = Fri May  1 14:41:00 2020\nOutput_Path = katana2:/home/z1234567/src/job.pbs.o315236\n    Priority = 0\nqtime = Fri May  1 14:41:00 2020\nRerunable = True\n    Resource_List.ib = no\n    Resource_List.mem = 1gb\n    Resource_List.ncpus = 1\nResource_List.ngpus = 0\nResource_List.nodect = 1\nResource_List.place = pack\n    Resource_List.select = 1:mem=1gb:ncpus=1\nResource_List.walltime = 01:00:00\n    substate = 10\nVariable_List = PBS_O_HOME=/home/z1234567,PBS_O_LANG=en_AU.UTF-8,\n        PBS_O_LOGNAME=z1234567,\n        PBS_O_PATH=/home/z1234567/bin:/usr/lib64/qt-3.3/bin:/usr/lib64/ccache:\n        /usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/pbs/bin,PBS_O_M\n        AIL=/var/spool/mail/z1234567,PBS_O_SHELL=/bin/bash,PBS_O_WORKDIR=/home\n        /z1234567/src,PBS_O_SYSTEM=Linux,PBS_O_QUEUE=submission,PBS_O_HOST=kat\n        ana2\n    etime = Fri May  1 14:41:00 2020\neligible_time = 00:00:00\n    Submit_arguments = -W group_list=GENERAL -N job.pbs job.pbs.JAZDNgL\n    project = _pbs_project_default\n</code></pre><p></p> <p>Remove a job from the queue or kill it if it's started. To remove an array job, you must include the square braces and they will need to be escaped. In that situation you use <code>qdel 12345\\[\\]</code>. Uses the <code>$JOBID</code> </p> <pre><code>[z1234567@katana2 src]$ qdel 315252\n</code></pre> <p>Once a job has been submitted, it can be altered. However, once a job begins execution, the only values that can be modified are <code>cputime</code>, <code>walltime</code>, and <code>run_count</code>. These can only be reduced.</p> <p>Users can only lower resource requests on queued jobs. If you need to increase resources, contact a systems administrator. In this example you will see the resources change - but not the <code>Submit_arguments</code></p> <pre><code>[z1234567@katana2 src]$ qsub -l select=1:ncpus=2:mem=128mb job.pbs\n315259.kman.restech.unsw.edu.au\n[z1234567@katana2 src]$ qstat -f 315259\nJob Id: 315259.kman.restech.unsw.edu.au\n    ...\n    Resource_List.mem = 128mb\n    Resource_List.ncpus = 2\n...\n    Submit_arguments = -W group_list=GENERAL -N job.pbs -l select=1:ncpus=2:mem=128mb job.pbs.YOOu3lB\n    project = _pbs_project_default\n\n[z1234567@katana2 src]$ qalter -l select=1:ncpus=4:mem=512mb 315259; qstat -f 315259\nJob Id: 315259.kman.restech.unsw.edu.au\n    ...\n    Resource_List.mem = 512mb\n    Resource_List.ncpus = 4\n...\n    Submit_arguments = -W group_list=GENERAL -N job.pbs -l select=1:ncpus=2:mem=128mb job.pbs.YOOu3lB\n    project = _pbs_project_default\n</code></pre>"},{"location":"using_katana/monitor_jobs/#job-stats","title":"Job Stats","text":"<p>As soon as your job finishes, PBS produces job statistics along with a summary of your job. This summary appears as follows (replace <code>4638435.kman.restech.unsw.edu.au.OU</code> for your output file; the steps for retrieving the file name are outlined below): </p><pre><code>z123456@katana2:~ $ cat 4638435.kman.restech.unsw.edu.au.OU\n\n================================================================================\nResource Usage on 27/07/2023 15:43:37\n\nJob Id: 4638435\nQueue: CSE\nWalltime: 00:00:03  (requested 01:00:00)\nJob execution was successful. Exit Status 0.\n\n--------------------------------------------------------------------------------\n|              |             CPUs              |            Memory             |\n--------------------------------------------------------------------------------\n| Node         | Requested   Used   Efficiency | Requested   Used   Efficiency |\n| k080         |     1        0.0      0.0%    |   1.0gb    0.01gb     1.0%    |\n--------------------------------------------------------------------------------\n</code></pre> This text is appended at the end of your output file. If you haven't specified an output file, one will be generated for you and placed in the folder where you submit your job.<p></p> <p>If you're unsure about the location of your job statistics file, you can run the following command (replace 4682962 with your job ID): </p><pre><code>z123456@katana2:~ $ qstat -xf 4682962\n</code></pre> then search for Output_Path. For example, in below example, the absolute path of the output file is <code>/home/z123456/output_file.txt</code>. Interactive jobs do no show a file name, for those cases the output file name look as <code>4638435.kman.restech.unsw.edu.au.OU</code> and is placed on the folder where you submitted the job: <pre><code>z3536424@katana2:~/hacky $ qstat -xf 4686875\nJob Id: 4686875.kman.restech.unsw.edu.au\n    Job_Name = hacky\n    Job_Owner = z3536424@katana2\n    resources_used.cpupercent = 0\nresources_used.cput = 00:00:00\n    resources_used.mem = 6896kb\n    resources_used.ncpus = 1\nresources_used.vmem = 6896kb\n    resources_used.walltime = 00:00:09\n    job_state = F\n    queue = cse12\n    server = kman\n    Checkpoint = u\n    ctime = Thu Aug 10 16:10:39 2023\nError_Path = /dev/pts/0\n    exec_host = k242/3\n    exec_vnode = (k242:ncpus=1:mem=1048576kb:ngpus=0)\nHold_Types = n\n    interactive = True\n    Join_Path = n\n    Keep_Files = n\n    Mail_Points = a\n    mtime = Thu Aug 10 16:11:09 2023\nOutput_Path = /home/z123456/output_file.txt\n    Priority = 0\nqtime = Thu Aug 10 16:10:39 2023\nRerunable = False\n    Resource_List.ib = no\n    Resource_List.mem = 1gb\n    Resource_List.ncpus = 1\nResource_List.ngpus = 0\nResource_List.nodect = 1\nResource_List.place = group=cse12\n    Resource_List.select = ncpus=1:mem=1gb\n    Resource_List.walltime = 01:00:00\n    stime = Thu Aug 10 16:10:55 2023\nobittime = Thu Aug 10 16:11:09 2023\nsession_id = 2621006\njobdir = /home/z3536424\n    substate = 92\nVariable_List = PBS_O_HOME=/home/z3536424,PBS_O_LANG=en_AU.UTF-8,\n    PBS_O_LOGNAME=z3536424,\n    PBS_O_PATH=/home/z3536424/.local/bin:/usr/share/Modules/bin:/usr/lib64\n    /ccache:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/pbs/bin,\n    PBS_O_MAIL=/var/spool/mail/z3536424,PBS_O_SHELL=/bin/bash,\n    PBS_O_WORKDIR=/home/z3536424/hacky,PBS_O_SYSTEM=Linux,\n    PBS_O_QUEUE=cse12,PBS_O_HOST=katana2\n    comment = Job run at Thu Aug 10 at 16:10 on (k242:ncpus=1:mem=1048576kb:ngp\n    us=0) and finished\n    etime = Thu Aug 10 16:10:39 2023\nrun_count = 1\neligible_time = 00:00:19\n    Exit_status = 0\nSubmit_arguments = -N hacky -I\n    history_timestamp = 1691647869\nproject = _pbs_project_default\n    Submit_Host = katana2\n</code></pre><p></p>"},{"location":"using_katana/ondemand/","title":"Web access to Katana","text":""},{"location":"using_katana/ondemand/#katana-ondemand","title":"Katana OnDemand","text":"<p>Katana OnDemand gives you web interface to Katana</p> <p>If you are not on campus, please make sure that you are connected to the UNSW VPN.</p> Katana OnDemand"},{"location":"using_katana/ondemand/#access-your-files","title":"Access your files","text":"<p>On Katana OnDemand, you can easily upload, download, delete and modify files</p> <p></p>"},{"location":"using_katana/ondemand/#interactive-apps","title":"Interactive Apps","text":"<p>You can run Virtual Desktop giving you access to multiple graphical applications, JupyterLab, RStudio, and a Terminal Session of Katana on your web browser.</p> <p>The status of your interactive sessions can be seen using the rightmost 'My Interactive Sessions' icon. </p>      Your browser does not support the video tag."},{"location":"using_katana/running_jobs/","title":"Running Jobs on Katana","text":""},{"location":"using_katana/running_jobs/#running-jobs-on-katana","title":"Running Jobs on Katana","text":""},{"location":"using_katana/running_jobs/#brief-overview","title":"Brief Overview","text":"<p>Katana is a High Performance Computing (HPC) cluster. It allows users to run computationally intensive programs on powerful compute nodes. Login nodes are only for preparing, submitting, and managing jobs\u2014not for heavy calculations.</p> <p> </p> Simple HPC Architecture <p>\u26a0 Warning: Do not run computationally intensive processes on login nodes; use compute nodes instead.</p> <p>Jobs are submitted from the login node, which delivers them to the Head Node for job and resource management. Once resources are allocated, the job will run on one or more compute nodes.</p> <p>Katana uses OpenPBS to manage resources and schedule jobs.</p>"},{"location":"using_katana/running_jobs/#job-types","title":"Job Types","text":"<ul> <li>Batch Job \u2013 Runs a scripted job automatically from start to finish without user intervention. Ideal for long-running production tasks.</li> <li>Interactive Job \u2013 Provides a live shell session on compute nodes for experimentation and debugging. Useful for testing and planning batch jobs.  </li> </ul> <p>All jobs enter a queue while waiting for resources.</p>"},{"location":"using_katana/running_jobs/#resources","title":"Resources","text":"<p>Main resources requested by jobs:</p> <ul> <li>Memory (RAM) </li> <li>CPU cores </li> <li>Walltime (time for CPUs)  </li> </ul> <p>Note: Increasing memory, CPU cores, or walltime may limit the available queues. See the queue limits below.</p>"},{"location":"using_katana/running_jobs/#create-a-job-folder","title":"Create a Job Folder","text":"<p>Create a dedicated folder for your job files. This keeps your work organized and makes it easier to manage multiple jobs.</p> <p></p><pre><code>mkdir MyFirstJob\n</code></pre> Then navigate into the folder:<p></p> <pre><code>cd MyFirstJob\n</code></pre>"},{"location":"using_katana/running_jobs/#create-a-script-or-program-to-run","title":"Create a script or program to run","text":"<p>Before submitting a job, you need a script or program to execute. This could be a compiled binary, a Python script, an R script, or any other executable file. For example, create a simple Python script named <code>myprogram.py</code>: </p><pre><code>nano myprogram.py\n</code></pre><p></p> <p>Inside nano, add the following code:</p> <p></p><pre><code>print(\"Hello, Katana!\")\n</code></pre> Save this file and exit nano (CTRL+S, ENTER, CTRL+X).<p></p>"},{"location":"using_katana/running_jobs/#batch-jobs-qsub","title":"Batch Jobs (qsub)","text":"<p>A batch job is a script that runs autonomously on a compute node. The script specifies resources and commands to run. Now you will create a job script to run the program you created.</p>"},{"location":"using_katana/running_jobs/#step-1-create-a-job-script-file","title":"Step 1: Create a Job Script File","text":"<pre><code># Create a new file called myjob.pbs\nnano myjob.pbs\n</code></pre> Windows Powershell <p>This opens a simple text editor in the terminal:</p> <p>Copy the following template into the editor:</p> <pre><code>#!/bin/bash\n\n#PBS -l select=1:ncpus=1:mem=4gb\n#PBS -l walltime=12:00:00\n#PBS -M zID@ad.unsw.edu.au\n#PBS -m ae\n#PBS -j oe\n\ncd $PBS_O_WORKDIR\n\npython3 myprogram.py\n</code></pre> <p>Note</p> <p>What each line does:</p> <pre><code>#!/bin/bash\n</code></pre> <p>Tells the system to use the Bash shell to run your script. Every script should start with this line.</p> <pre><code>#PBS -l select=1:ncpus=1:mem=4gb`\n</code></pre> <p>Requests 1 compute node with 1 CPU core and 4GB RAM. The scheduler uses this to allocate resources. You can adjust <code>ncpus</code> and <code>mem</code> as needed.</p> <pre><code>#PBS -l walltime=12:00:00\n</code></pre> <p>Sets the maximum run time to 12 hours. See the Walltime section below for more details.</p> <pre><code>#PBS -M your.name.here@unsw.edu.au\n#PBS -m ae\n</code></pre> <p>(Optional)Sends an email notification if the job aborts (a) or ends normally (e). Useful to know when your job finishes or fails. </p> <pre><code>#PBS -j oe\n</code></pre> <p>(Optional)Combines the standard output and standard error into a single file, making it easier to review the results.</p> <pre><code>cd $PBS_O_WORKDIR\n</code></pre> <p>Changes the working directory to where you ran qsub. This ensures you are running in the correct folder.</p> <pre><code>python3 myprogram.py\n</code></pre> <p>Runs your program. Replace myprogram with the actual program or script you want to execute.</p> <ul> <li>Press <code>CTRL+S</code> to save and <code>CTRL+X</code> to exit <code>nano</code>.</li> </ul>"},{"location":"using_katana/running_jobs/#step-2-submit-the-batch-job","title":"Step 2: Submit the Batch Job","text":"<pre><code>qsub myjob.pbs\n</code></pre> <ul> <li>Terminal will return a job ID (e.g., <code>1239.kman.restech.unsw.edu.au</code>)  </li> <li>Scheduler will run the job when resources are available.</li> <li>When the job is finished, two files will be created in your folder: <code>myjob.o1239</code> and <code>myjob.e1239</code> (where <code>1239</code> is your job ID). The <code>.o</code> file contains standard output, and the <code>.e</code> file contains any error messages. You can use <code>cat</code>, <code>less</code>, or <code>nano</code> to view these files.</li> <li>To check the status of your job, use:</li> </ul> <pre><code>qstat job_id (e.g., qstat 6787878)\n</code></pre>"},{"location":"using_katana/running_jobs/#interactive-jobs-qsub-i","title":"Interactive Jobs (qsub -I)","text":"<p>Interactive jobs let you run commands directly on Katana\u2019s compute nodes (not the login node). This is useful when:</p> <ul> <li>You need to test software before creating a batch script.</li> <li>You want to debug or profile code in a live environment.</li> <li>You are running short, exploratory tasks where you need to see output in real time.</li> </ul> <p>Note</p> <p>Interactive jobs are not suitable for long-running tasks. Use batch jobs for production workloads. Do not run heavy computations on the login node. Always request an interactive session so your work runs on compute resources.</p>"},{"location":"using_katana/running_jobs/#starting-an-interactive-session","title":"Starting an Interactive Session","text":"<p>To start an interactive job, use qsub -I with resource requests. For example:</p> <pre><code>qsub -I \nor \nqsub -I -l select=1:ncpus=2:mem=8gb -l walltime=02:00:00\n</code></pre> <p>Mote</p> <p>Without resource requests, the default is 1 CPU core, 4GB RAM, and 1 hour walltime.</p> <p>Once your session starts, you will be logged into a compute node. You can run commands as you would on the login node.</p>"},{"location":"using_katana/running_jobs/#understanding-walltime","title":"Understanding Walltime","text":"<p>Walltime is the maximum amount of real time that your job is allowed to run on the cluster. It is requested when you submit a job and is used by the scheduler to plan resources.</p> <ul> <li>Walltime is specified in hours, minutes, and seconds, the default walltime is 1 hour if not specified in script.</li> <li>If your job runs longer than the walltime, it will be terminated automatically, even if it hasn\u2019t finished.</li> <li>Walltime affects which queues your job can be scheduled on. Shorter walltime jobs usually start faster, while longer jobs may only be able to run on specific nodes.</li> <li>Always estimate your job\u2019s runtime carefully. If unsure, it\u2019s safer to slightly overestimate but not excessively, as very long walltime requests may reduce scheduling priority.</li> <li>For long workflows, consider splitting tasks into multiple jobs to fit within walltime limits.</li> </ul>"},{"location":"using_katana/running_jobs/#job-queue-limits-summary","title":"Job queue limits summary","text":"<p>Typical job queue limit cut-offs are shown below. The walltime is what determines whether a job can be run on any node, or only on a restricted set of nodes.</p> Resource Queue limit cut-offs Memory (GB) 124 180 248 370 750 1000 CPU Cores 16 20 24 28 32 44 Walltime (hrs) 12 48 100 200 Any node School-owned or general-use nodes School-owned nodes only"},{"location":"using_katana/running_jobs/#restech-github-repositories","title":"Restech GitHub Repositories","text":"<ul> <li>Restech-HPC \u2013 Example Katana scripts  </li> <li>UNSW-Data-Archive \u2013 Upload/download scripts  </li> <li>UNSW-eNotebook-LabArchives \u2013 LabArchives widgets</li> </ul>"},{"location":"using_katana/tips_katana/","title":"Tips for using PBS and Katana effectively","text":""},{"location":"using_katana/tips_katana/#keep-your-jobs-under-12-hours-if-possible","title":"Keep your jobs under 12 hours if possible","text":"<p>If you request more than 12 hours of <code>WALLTIME</code> then you can only use the nodes bought by your school or research group. Keeping your job's run time request under 12 hours means that it can run on any node in the cluster.</p> <p>Important</p> <p>Two 10 hour jobs will probably finish sooner that one 20 hour job.</p> <p>In fact, if there is spare capacity on Katana, which there is most of the time, six 10 hours jobs will finish before a single 20 hour job will. Requesting more resources for your job decreases the places that the job can run</p> <p>The most obvious example is going over the 12 hour limit which limits the number of compute nodes that your job can run on. For example specifying the CPU in your job script restricts you to the nodes with that CPU. A job that requests 20Gb will run on a 128Gb node with a 100Gb job already running but a 30Gb job will not be able to.</p>"},{"location":"using_katana/tips_katana/#running-your-jobs-interactively-makes-it-hard-to-manage-multiple-concurrent-jobs","title":"Running your jobs interactively makes it hard to manage multiple concurrent jobs","text":"<p>If you are currently only running jobs interactively then you should move to batch jobs which allow you to submit more jobs which then start, run and finish automatically. If you have multiple batch jobs that are almost identical then you should consider using array jobs</p> <p>If your batch jobs are the same except for a change in file name or another variable then you should have a look at using array jobs.</p>"},{"location":"using_katana/tips_katana/#other-advanced-usages","title":"Other Advanced Usages","text":"<p>As well as the information presented here, examples are available in the Restech HPC Github repository</p>"},{"location":"using_katana/tips_katana/#array-jobs","title":"Array Jobs","text":"<p>One common use of computational clusters is to do the same thing multiple times - sometimes with slightly different input, sometimes to get averages from randomness within the process. This is made easier with array jobs.</p> <p>An array job is a single job script that spawns many almost identical sub-jobs. The only difference between the sub-jobs is an environment variable <code>$PBS_ARRAY_INDEX</code> whose value uniquely identifies an individual sub-job. A regular job becomes an array job when it uses the <code>#PBS -J</code> flag. </p> <p>For example, the following script will spawn 100 sub-jobs. Each sub-job will require one CPU core, 1GB memory and 1 hour run-time, and it will execute the same application. However, a different input file will be passed to the application within each sub-job. The first sub-job will read input data from a file called <code>1.dat</code>, the second sub-job will read input data from a file called <code>2.dat</code> and so on. </p> <p>Note</p> <p>In this example we are using <code>brace expansion</code> - the {} characters around the bash variables - because they are needed for variables that change, like array indices. They aren't strictly necessary for <code>$PBS_O_WORKDIR</code> but we include them to show consistency.</p> <pre><code>#!/bin/bash\n\n#PBS -l select=1:ncpus=1:mem=1gb\n#PBS -l walltime=1:00:00\n#PBS -j oe\n#PBS -J 1-100\n\ncd ${PBS_O_WORKDIR}\n\n./myprogram ${PBS_ARRAY_INDEX}.dat\n</code></pre> <p>There are some more examples of array jobs including how to group your computations in an array job on the UNSW Github HPC examples page. (Note: You need to join the UNSW GitHub organisation to access this repo)</p>"},{"location":"using_katana/tips_katana/#splitting-large-batch-jobs","title":"Splitting large Batch Jobs","text":"<p>If your batch job can be split into multiple steps you may want to split one big job up into a number of smaller jobs. There are a number of reasons to spend the time to implement this.</p> <ol> <li>If your large job runs for over 200 hours, it won't finish on Katana.</li> <li>If your job has multiple steps which use different amounts of resources at each step. If you have a pipeline that takes 50 hours to run and needs 200GB of memory for an hour, but only 50GB the rest of the time, then the memory is sitting idle. </li> <li>Katana has prioritisations based on how many resources any one user uses. If you ask for 200GB of memory, this will be accounted for when working out your next job's priority.</li> <li>Because there are many more resources for 12 hour jobs, seven or eight 12 hour jobs will often finish well before a single 100 hour job even starts. </li> </ol>"}]}